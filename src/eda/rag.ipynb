{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riley/base/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb import Settings\n",
    "import os\n",
    "from collections import Counter\n",
    "import kagglehub\n",
    "import re\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from data_loading.load_luflow import get_luflow\n",
    "from data_loading.tools import reduce_mem_usage\n",
    "\n",
    "vector_db_path = os.path.join(os.getcwd(), os.pardir, os.pardir, \"data/vector_db\")\n",
    "client = chromadb.PersistentClient(path=vector_db_path, settings=Settings(allow_reset=True, \n",
    "                                                                          anonymized_telemetry=False))\n",
    "\n",
    "use_luflow = False\n",
    "use_uq = True\n",
    "collection_name = \"packet_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 20.35 MB\n",
      "Decreased by 57.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riley/base/lib/python3.10/site-packages/pandas/io/formats/format.py:1458: RuntimeWarning: overflow encountered in cast\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/home/riley/base/lib/python3.10/site-packages/pandas/io/formats/format.py:1458: RuntimeWarning: overflow encountered in cast\n",
      "  has_large_values = (abs_vals > 1e6).any()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IPV4_SRC_ADDR</th>\n",
       "      <th>L4_SRC_PORT</th>\n",
       "      <th>IPV4_DST_ADDR</th>\n",
       "      <th>L4_DST_PORT</th>\n",
       "      <th>PROTOCOL</th>\n",
       "      <th>L7_PROTO</th>\n",
       "      <th>IN_BYTES</th>\n",
       "      <th>IN_PKTS</th>\n",
       "      <th>OUT_BYTES</th>\n",
       "      <th>OUT_PKTS</th>\n",
       "      <th>...</th>\n",
       "      <th>TCP_WIN_MAX_OUT</th>\n",
       "      <th>ICMP_TYPE</th>\n",
       "      <th>ICMP_IPV4_TYPE</th>\n",
       "      <th>DNS_QUERY_ID</th>\n",
       "      <th>DNS_QUERY_TYPE</th>\n",
       "      <th>DNS_TTL_ANSWER</th>\n",
       "      <th>FTP_COMMAND_RET_CODE</th>\n",
       "      <th>Label</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>192.168.100.148</td>\n",
       "      <td>65389</td>\n",
       "      <td>192.168.100.7</td>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>420</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>35840</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>DoS</td>\n",
       "      <td>NF-BoT-IoT-v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>192.168.100.148</td>\n",
       "      <td>11154</td>\n",
       "      <td>192.168.100.5</td>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>280</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>DoS</td>\n",
       "      <td>NF-BoT-IoT-v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>192.168.1.31</td>\n",
       "      <td>54001</td>\n",
       "      <td>192.168.1.180</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>92.0</td>\n",
       "      <td>84</td>\n",
       "      <td>2</td>\n",
       "      <td>88</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>29200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>scanning</td>\n",
       "      <td>NF-ToN-IoT-v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>192.168.100.147</td>\n",
       "      <td>33372</td>\n",
       "      <td>192.168.100.5</td>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>280</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>35840</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>DoS</td>\n",
       "      <td>NF-BoT-IoT-v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>192.168.1.31</td>\n",
       "      <td>37085</td>\n",
       "      <td>192.168.1.193</td>\n",
       "      <td>1863</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>scanning</td>\n",
       "      <td>NF-ToN-IoT-v2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     IPV4_SRC_ADDR  L4_SRC_PORT  IPV4_DST_ADDR  L4_DST_PORT  PROTOCOL  \\\n",
       "0  192.168.100.148        65389  192.168.100.7           80         6   \n",
       "1  192.168.100.148        11154  192.168.100.5           80         6   \n",
       "7     192.168.1.31        54001  192.168.1.180           22         6   \n",
       "8  192.168.100.147        33372  192.168.100.5           80         6   \n",
       "9     192.168.1.31        37085  192.168.1.193         1863         6   \n",
       "\n",
       "   L7_PROTO  IN_BYTES  IN_PKTS  OUT_BYTES  OUT_PKTS  ...  TCP_WIN_MAX_OUT  \\\n",
       "0       7.0       420        3          0         0  ...                0   \n",
       "1       7.0       280        2         40         1  ...                0   \n",
       "7      92.0        84        2         88         2  ...            29200   \n",
       "8       7.0       280        2         40         1  ...                0   \n",
       "9       0.0        44        1         40         1  ...                0   \n",
       "\n",
       "   ICMP_TYPE  ICMP_IPV4_TYPE  DNS_QUERY_ID  DNS_QUERY_TYPE  DNS_TTL_ANSWER  \\\n",
       "0      35840             140             0               0               0   \n",
       "1          0               0             0               0               0   \n",
       "7          0               0             0               0               0   \n",
       "8      35840             140             0               0               0   \n",
       "9          0               0             0               0               0   \n",
       "\n",
       "   FTP_COMMAND_RET_CODE  Label    Attack        Dataset  \n",
       "0                   0.0      1       DoS  NF-BoT-IoT-v2  \n",
       "1                   0.0      1       DoS  NF-BoT-IoT-v2  \n",
       "7                   0.0      1  scanning  NF-ToN-IoT-v2  \n",
       "8                   0.0      1       DoS  NF-BoT-IoT-v2  \n",
       "9                   0.0      1  scanning  NF-ToN-IoT-v2  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if use_luflow:\n",
    "    data = get_luflow(raw=True)\n",
    "else:\n",
    "    # Path to the cached dataset\n",
    "    cache_path = os.path.expanduser(\"~/.cache/kagglehub/datasets\")\n",
    "    data_path = os.path.join(cache_path, \"aryashah2k/nfuqnidsv2-network-intrusion-detection-dataset/versions/1\")\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        # Download latest version\n",
    "        data_path = kagglehub.dataset_download(\"aryashah2k/nfuqnidsv2-network-intrusion-detection-dataset\")\n",
    "    \n",
    "    data = pd.read_csv(os.path.join(data_path, \"NF-UQ-NIDS-v2.csv\"), nrows=200_000)\n",
    "\n",
    "data = data[data[\"Label\"] != 0]\n",
    "data = reduce_mem_usage(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attack\n",
       "DDoS              57505\n",
       "DoS               47267\n",
       "scanning           9947\n",
       "Reconnaissance     6935\n",
       "xss                6410\n",
       "password           3071\n",
       "injection          1809\n",
       "Bot                 371\n",
       "Brute Force         344\n",
       "Infilteration       315\n",
       "Exploits             77\n",
       "Fuzzers              56\n",
       "Backdoor             52\n",
       "Generic              33\n",
       "mitm                 19\n",
       "ransomware           10\n",
       "Analysis              7\n",
       "Theft                 3\n",
       "Shellcode             3\n",
       "Worms                 1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Attack'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under-sampled class distribution: Counter({'DoS': 57505, 'scanning': 57505, 'DDoS': 57505, 'xss': 57505, 'Bot': 57505, 'Reconnaissance': 57505, 'password': 57505, 'Fuzzers': 57505, 'injection': 57505, 'Theft': 57505, 'Brute Force': 57505, 'Infilteration': 57505, 'Exploits': 57505, 'Generic': 57505, 'Analysis': 57505, 'Backdoor': 57505, 'mitm': 57505, 'Shellcode': 57505, 'ransomware': 57505, 'Worms': 57505})\n"
     ]
    }
   ],
   "source": [
    "X = data.drop(columns=['Attack'])\n",
    "y = data['Attack']\n",
    "oversampler = RandomOverSampler(sampling_strategy='all')\n",
    "X_over, y_over = oversampler.fit_resample(X, y)\n",
    "print(f\"Under-sampled class distribution: {Counter(y_over)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1150100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115010"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only keep 10% of the oversampled data\n",
    "X_over = X_over.sample(frac=0.1)\n",
    "y_over = y_over[X_over.index]\n",
    "len(X_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riley/base/lib/python3.10/site-packages/pandas/io/formats/format.py:1458: RuntimeWarning: overflow encountered in cast\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/home/riley/base/lib/python3.10/site-packages/pandas/io/formats/format.py:1458: RuntimeWarning: overflow encountered in cast\n",
      "  has_large_values = (abs_vals > 1e6).any()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IPV4_SRC_ADDR</th>\n",
       "      <th>L4_SRC_PORT</th>\n",
       "      <th>IPV4_DST_ADDR</th>\n",
       "      <th>L4_DST_PORT</th>\n",
       "      <th>PROTOCOL</th>\n",
       "      <th>L7_PROTO</th>\n",
       "      <th>IN_BYTES</th>\n",
       "      <th>IN_PKTS</th>\n",
       "      <th>OUT_BYTES</th>\n",
       "      <th>OUT_PKTS</th>\n",
       "      <th>...</th>\n",
       "      <th>TCP_WIN_MAX_OUT</th>\n",
       "      <th>ICMP_TYPE</th>\n",
       "      <th>ICMP_IPV4_TYPE</th>\n",
       "      <th>DNS_QUERY_ID</th>\n",
       "      <th>DNS_QUERY_TYPE</th>\n",
       "      <th>DNS_TTL_ANSWER</th>\n",
       "      <th>FTP_COMMAND_RET_CODE</th>\n",
       "      <th>Label</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>356817</th>\n",
       "      <td>18.221.219.4</td>\n",
       "      <td>33540</td>\n",
       "      <td>172.31.69.25</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>780</td>\n",
       "      <td>13</td>\n",
       "      <td>520</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NF-CSE-CIC-IDS2018-v2</td>\n",
       "      <td>Brute Force</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028813</th>\n",
       "      <td>192.168.1.33</td>\n",
       "      <td>44501</td>\n",
       "      <td>192.168.1.193</td>\n",
       "      <td>445</td>\n",
       "      <td>6</td>\n",
       "      <td>41.0</td>\n",
       "      <td>7317</td>\n",
       "      <td>24</td>\n",
       "      <td>304</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>8192</td>\n",
       "      <td>22528</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NF-ToN-IoT-v2</td>\n",
       "      <td>ransomware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310517</th>\n",
       "      <td>13.58.98.64</td>\n",
       "      <td>42240</td>\n",
       "      <td>172.31.69.25</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>92.0</td>\n",
       "      <td>3184</td>\n",
       "      <td>24</td>\n",
       "      <td>3869</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>26847</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NF-CSE-CIC-IDS2018-v2</td>\n",
       "      <td>Brute Force</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174057</th>\n",
       "      <td>175.45.176.0</td>\n",
       "      <td>0</td>\n",
       "      <td>149.171.126.17</td>\n",
       "      <td>0</td>\n",
       "      <td>216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NF-UNSW-NB15-v2</td>\n",
       "      <td>Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335670</th>\n",
       "      <td>13.58.98.64</td>\n",
       "      <td>57588</td>\n",
       "      <td>172.31.69.25</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>92.0</td>\n",
       "      <td>3116</td>\n",
       "      <td>23</td>\n",
       "      <td>3869</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>26847</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NF-CSE-CIC-IDS2018-v2</td>\n",
       "      <td>Brute Force</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        IPV4_SRC_ADDR  L4_SRC_PORT   IPV4_DST_ADDR  L4_DST_PORT  PROTOCOL  \\\n",
       "356817   18.221.219.4        33540    172.31.69.25           21         6   \n",
       "1028813  192.168.1.33        44501   192.168.1.193          445         6   \n",
       "310517    13.58.98.64        42240    172.31.69.25           22         6   \n",
       "174057   175.45.176.0            0  149.171.126.17            0       216   \n",
       "335670    13.58.98.64        57588    172.31.69.25           22         6   \n",
       "\n",
       "         L7_PROTO  IN_BYTES  IN_PKTS  OUT_BYTES  OUT_PKTS  ...  \\\n",
       "356817        1.0       780       13        520        13  ...   \n",
       "1028813      41.0      7317       24        304         6  ...   \n",
       "310517       92.0      3184       24       3869        23  ...   \n",
       "174057        0.0       200        2          0         0  ...   \n",
       "335670       92.0      3116       23       3869        23  ...   \n",
       "\n",
       "         TCP_WIN_MAX_OUT  ICMP_TYPE  ICMP_IPV4_TYPE  DNS_QUERY_ID  \\\n",
       "356817                 0          0               0             0   \n",
       "1028813             8192      22528              88             0   \n",
       "310517             26847          0               0             0   \n",
       "174057                 0          0               0             0   \n",
       "335670             26847          0               0             0   \n",
       "\n",
       "         DNS_QUERY_TYPE  DNS_TTL_ANSWER  FTP_COMMAND_RET_CODE  Label  \\\n",
       "356817                0               0                   0.0      1   \n",
       "1028813               0               0                   0.0      1   \n",
       "310517                0               0                   0.0      1   \n",
       "174057                0               0                   0.0      1   \n",
       "335670                0               0                   0.0      1   \n",
       "\n",
       "                       Dataset       Attack  \n",
       "356817   NF-CSE-CIC-IDS2018-v2  Brute Force  \n",
       "1028813          NF-ToN-IoT-v2   ransomware  \n",
       "310517   NF-CSE-CIC-IDS2018-v2  Brute Force  \n",
       "174057         NF-UNSW-NB15-v2     Analysis  \n",
       "335670   NF-CSE-CIC-IDS2018-v2  Brute Force  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = X_over\n",
    "data['Attack'] = y_over\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under-sampled class distribution: Counter({'Worms': 5867, 'mitm': 5853, 'password': 5852, 'Backdoor': 5820, 'Exploits': 5799, 'Shellcode': 5796, 'Theft': 5778, 'Brute Force': 5756, 'Generic': 5756, 'Bot': 5755, 'xss': 5753, 'Reconnaissance': 5736, 'Analysis': 5736, 'DoS': 5714, 'Fuzzers': 5712, 'Infilteration': 5710, 'ransomware': 5702, 'scanning': 5685, 'injection': 5650, 'DDoS': 5580})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Under-sampled class distribution: {Counter(y_over)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['IPV4_SRC_ADDR', 'L4_SRC_PORT', 'IPV4_DST_ADDR', 'L4_DST_PORT',\n",
       "       'PROTOCOL', 'L7_PROTO', 'IN_BYTES', 'IN_PKTS', 'OUT_BYTES', 'OUT_PKTS',\n",
       "       'TCP_FLAGS', 'CLIENT_TCP_FLAGS', 'SERVER_TCP_FLAGS',\n",
       "       'FLOW_DURATION_MILLISECONDS', 'DURATION_IN', 'DURATION_OUT', 'MIN_TTL',\n",
       "       'MAX_TTL', 'LONGEST_FLOW_PKT', 'SHORTEST_FLOW_PKT', 'MIN_IP_PKT_LEN',\n",
       "       'MAX_IP_PKT_LEN', 'SRC_TO_DST_SECOND_BYTES', 'DST_TO_SRC_SECOND_BYTES',\n",
       "       'RETRANSMITTED_IN_BYTES', 'RETRANSMITTED_IN_PKTS',\n",
       "       'RETRANSMITTED_OUT_BYTES', 'RETRANSMITTED_OUT_PKTS',\n",
       "       'SRC_TO_DST_AVG_THROUGHPUT', 'DST_TO_SRC_AVG_THROUGHPUT',\n",
       "       'NUM_PKTS_UP_TO_128_BYTES', 'NUM_PKTS_128_TO_256_BYTES',\n",
       "       'NUM_PKTS_256_TO_512_BYTES', 'NUM_PKTS_512_TO_1024_BYTES',\n",
       "       'NUM_PKTS_1024_TO_1514_BYTES', 'TCP_WIN_MAX_IN', 'TCP_WIN_MAX_OUT',\n",
       "       'ICMP_TYPE', 'ICMP_IPV4_TYPE', 'DNS_QUERY_ID', 'DNS_QUERY_TYPE',\n",
       "       'DNS_TTL_ANSWER', 'FTP_COMMAND_RET_CODE', 'Label', 'Dataset', 'Attack'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_packet_documents_luflow(df):\n",
    "    documents = []\n",
    "    \n",
    "    print(\"Preparing packet documents...\")\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        # Create a structured text representation of each flow\n",
    "        doc_parts = []\n",
    "        doc_parts.append(f\"Flow ID: {idx}\")\n",
    "        \n",
    "        # Basic flow metrics\n",
    "        doc_parts.append(f\"Average IPT: {row.get('avg_ipt', 'unknown')}\")\n",
    "        doc_parts.append(f\"Bytes In: {row.get('bytes_in', 'unknown')}\")\n",
    "        doc_parts.append(f\"Bytes Out: {row.get('bytes_out', 'unknown')}\")\n",
    "        \n",
    "        # Connection details\n",
    "        doc_parts.append(f\"Connection: {row.get('src_ip', 'unknown')}:{row.get('src_port', 'unknown')} → {row.get('dest_ip', 'unknown')}:{row.get('dest_port', 'unknown')}\")\n",
    "        \n",
    "        # Protocol information\n",
    "        doc_parts.append(f\"Protocol: {row.get('proto', 'unknown')}\")\n",
    "        \n",
    "        # Packet counts\n",
    "        doc_parts.append(f\"Packets In: {row.get('num_pkts_in', 'unknown')}\")\n",
    "        doc_parts.append(f\"Packets Out: {row.get('num_pkts_out', 'unknown')}\")\n",
    "        \n",
    "        # Entropy and statistical features\n",
    "        doc_parts.append(f\"Entropy: {row.get('entropy', 'unknown')}\")\n",
    "        doc_parts.append(f\"Total Entropy: {row.get('total_entropy', 'unknown')}\")\n",
    "        \n",
    "        # Timing information\n",
    "        doc_parts.append(f\"Start Time: {row.get('time_start', 'unknown')}\")\n",
    "        doc_parts.append(f\"End Time: {row.get('time_end', 'unknown')}\")\n",
    "        doc_parts.append(f\"Duration: {row.get('duration', 'unknown')}\")\n",
    "        \n",
    "        # Label and temporal context\n",
    "        doc_parts.append(f\"Label: {row.get('label', 'unknown')}\")\n",
    "        doc_parts.append(f\"Date: {row.get('Year', 'unknown')}-{row.get('Month', 'unknown')}-{row.get('Day', 'unknown')}\")\n",
    "        \n",
    "        # Combine all parts into a single document\n",
    "        document = \"\\n\".join(doc_parts)\n",
    "        \n",
    "        # Store the document with its metadata\n",
    "        documents.append({\n",
    "            \"id\": idx, \n",
    "            \"content\": document, \n",
    "            \"metadata\": {\n",
    "                \"avg_ipt\": row.get('avg_ipt', 0),\n",
    "                \"bytes_in\": row.get('bytes_in', 0),\n",
    "                \"bytes_out\": row.get('bytes_out', 0),\n",
    "                \"dest_ip\": row.get('dest_ip', ''),\n",
    "                \"dest_port\": row.get('dest_port', 0),\n",
    "                \"entropy\": row.get('entropy', 0),\n",
    "                \"num_pkts_out\": row.get('num_pkts_out', 0),\n",
    "                \"num_pkts_in\": row.get('num_pkts_in', 0),\n",
    "                \"proto\": row.get('proto', 0),\n",
    "                \"src_ip\": row.get('src_ip', ''),\n",
    "                \"src_port\": row.get('src_port', 0),\n",
    "                \"time_end\": row.get('time_end', 0),\n",
    "                \"time_start\": row.get('time_start', 0),\n",
    "                \"total_entropy\": row.get('total_entropy', 0),\n",
    "                \"label\": row.get('label', 0),\n",
    "                \"duration\": row.get('duration', 0),\n",
    "                \"year\": row.get('Year', 0),\n",
    "                \"month\": row.get('Month', 0),\n",
    "                \"day\": row.get('Day', 0)\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_packet_documents_uq(df):\n",
    "    documents = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Create a structured text representation of each flow\n",
    "        doc_parts = []\n",
    "        doc_parts.append(f\"Flow ID: {idx}\")\n",
    "        \n",
    "        # Connection details\n",
    "        doc_parts.append(f\"Connection: {row.get('IPV4_SRC_ADDR', 'unknown')}:{row.get('L4_SRC_PORT', 'unknown')} → {row.get('IPV4_DST_ADDR', 'unknown')}:{row.get('L4_DST_PORT', 'unknown')}\")\n",
    "        \n",
    "        # Protocol information\n",
    "        doc_parts.append(f\"Protocol: {row.get('PROTOCOL', 'unknown')} (L7: {row.get('L7_PROTO', 'unknown')})\")\n",
    "        \n",
    "        # Volume metrics\n",
    "        doc_parts.append(f\"Traffic Volume: {row.get('IN_BYTES', 0) + row.get('OUT_BYTES', 0)} bytes total ({row.get('IN_PKTS', 0) + row.get('OUT_PKTS', 0)} packets)\")\n",
    "        doc_parts.append(f\"Inbound: {row.get('IN_BYTES', 0)} bytes in {row.get('IN_PKTS', 0)} packets\")\n",
    "        doc_parts.append(f\"Outbound: {row.get('OUT_BYTES', 0)} bytes in {row.get('OUT_PKTS', 0)} packets\")\n",
    "        \n",
    "        # TCP specific information\n",
    "        if row.get('PROTOCOL') == 6:  # TCP\n",
    "            doc_parts.append(f\"TCP Flags: {row.get('TCP_FLAGS', 'unknown')}\")\n",
    "            doc_parts.append(f\"Client TCP Flags: {row.get('CLIENT_TCP_FLAGS', 'unknown')}\")\n",
    "            doc_parts.append(f\"Server TCP Flags: {row.get('SERVER_TCP_FLAGS', 'unknown')}\")\n",
    "            doc_parts.append(f\"TCP Window Max (In/Out): {row.get('TCP_WIN_MAX_IN', 'unknown')}/{row.get('TCP_WIN_MAX_OUT', 'unknown')}\")\n",
    "            \n",
    "            # Retransmission stats\n",
    "            doc_parts.append(f\"Retransmissions: {row.get('RETRANSMITTED_IN_PKTS', 0) + row.get('RETRANSMITTED_OUT_PKTS', 0)} packets ({row.get('RETRANSMITTED_IN_BYTES', 0) + row.get('RETRANSMITTED_OUT_BYTES', 0)} bytes)\")\n",
    "        \n",
    "        # ICMP specific information\n",
    "        if row.get('PROTOCOL') == 1:  # ICMP\n",
    "            doc_parts.append(f\"ICMP Type: {row.get('ICMP_TYPE', 'unknown')}\")\n",
    "            doc_parts.append(f\"ICMP IPv4 Type: {row.get('ICMP_IPV4_TYPE', 'unknown')}\")\n",
    "        \n",
    "        # DNS specific information\n",
    "        if row.get('DNS_QUERY_ID') is not None:\n",
    "            doc_parts.append(f\"DNS Query ID: {row.get('DNS_QUERY_ID', 'unknown')}\")\n",
    "            doc_parts.append(f\"DNS Query Type: {row.get('DNS_QUERY_TYPE', 'unknown')}\")\n",
    "            doc_parts.append(f\"DNS TTL Answer: {row.get('DNS_TTL_ANSWER', 'unknown')}\")\n",
    "        \n",
    "        # FTP specific information\n",
    "        if row.get('FTP_COMMAND_RET_CODE') is not None:\n",
    "            doc_parts.append(f\"FTP Return Code: {row.get('FTP_COMMAND_RET_CODE', 'unknown')}\")\n",
    "        \n",
    "        # Timing information\n",
    "        doc_parts.append(f\"Flow Duration: {row.get('FLOW_DURATION_MILLISECONDS', 'unknown')} ms\")\n",
    "        doc_parts.append(f\"Duration In/Out: {row.get('DURATION_IN', 'unknown')}/{row.get('DURATION_OUT', 'unknown')}\")\n",
    "        \n",
    "        # Throughput\n",
    "        doc_parts.append(f\"Throughput Client→Server: {row.get('SRC_TO_DST_AVG_THROUGHPUT', 'unknown')} bytes/s\")\n",
    "        doc_parts.append(f\"Throughput Server→Client: {row.get('DST_TO_SRC_AVG_THROUGHPUT', 'unknown')} bytes/s\")\n",
    "        \n",
    "        # Packet size distribution\n",
    "        doc_parts.append(\"Packet Size Distribution:\")\n",
    "        doc_parts.append(f\"  ≤128 bytes: {row.get('NUM_PKTS_UP_TO_128_BYTES', 0)} packets\")\n",
    "        doc_parts.append(f\"  128-256 bytes: {row.get('NUM_PKTS_128_TO_256_BYTES', 0)} packets\")\n",
    "        doc_parts.append(f\"  256-512 bytes: {row.get('NUM_PKTS_256_TO_512_BYTES', 0)} packets\")\n",
    "        doc_parts.append(f\"  512-1024 bytes: {row.get('NUM_PKTS_512_TO_1024_BYTES', 0)} packets\")\n",
    "        doc_parts.append(f\"  1024-1514 bytes: {row.get('NUM_PKTS_1024_TO_1514_BYTES', 0)} packets\")\n",
    "        \n",
    "        # TTL and packet lengths\n",
    "        doc_parts.append(f\"TTL Range: {row.get('MIN_TTL', 'unknown')}-{row.get('MAX_TTL', 'unknown')}\")\n",
    "        doc_parts.append(f\"Packet Length Range: {row.get('MIN_IP_PKT_LEN', 'unknown')}-{row.get('MAX_IP_PKT_LEN', 'unknown')}\")\n",
    "        doc_parts.append(f\"Shortest/Longest Packet: {row.get('SHORTEST_FLOW_PKT', 'unknown')}/{row.get('LONGEST_FLOW_PKT', 'unknown')}\")\n",
    "        \n",
    "        # Classification information\n",
    "        if 'Label' in row:\n",
    "            doc_parts.append(f\"Label: {row.get('Label', 'unknown')}\")\n",
    "        if 'Attack' in row:\n",
    "            doc_parts.append(f\"Attack: {row.get('Attack', 'unknown')}\")\n",
    "        if 'Dataset' in row:\n",
    "            doc_parts.append(f\"Dataset: {row.get('Dataset', 'unknown')}\")\n",
    "        \n",
    "        # Combine all parts into a single document\n",
    "        document = \"\\n\".join(doc_parts)\n",
    "        \n",
    "        # Store the document with its metadata\n",
    "        # Convert metadata to appropriate types to avoid issues\n",
    "        metadata = {}\n",
    "        for col in row.index:\n",
    "            value = row.get(col)\n",
    "            # Handle NaN values and convert to basic types that ChromaDB can handle\n",
    "            if pd.isna(value):\n",
    "                continue\n",
    "            elif isinstance(value, (int, float, str, bool)):\n",
    "                metadata[col] = value\n",
    "            else:\n",
    "                # Convert other types to strings\n",
    "                metadata[col] = str(value)\n",
    "        \n",
    "        documents.append({\n",
    "            \"id\": idx, \n",
    "            \"content\": document, \n",
    "            \"metadata\": metadata\n",
    "        })\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(documents):\n",
    "    # Load a pretrained model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    print(\"Extracting text content from documents...\")\n",
    "    # Extract just the text content\n",
    "    texts = [doc[\"content\"] for doc in tqdm(documents)]\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = model.encode(texts)\n",
    "    \n",
    "    print(f\"Generated {len(embeddings)} embeddings\")\n",
    "    print(\"Adding embeddings to documents...\")\n",
    "    # Add embeddings to documents\n",
    "    for i, doc in tqdm(enumerate(documents)):\n",
    "        doc[\"embedding\"] = embeddings[i]\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_in_vector_db(documents, client):\n",
    "    # Create or get a collection\n",
    "    collection = client.get_or_create_collection(\"packet_data\")\n",
    "    \n",
    "    # Prepare data for insertion\n",
    "    ids = [str(doc[\"id\"]) for doc in documents]\n",
    "    embeddings = [doc[\"embedding\"].tolist() for doc in documents]\n",
    "    metadatas = [doc[\"metadata\"] for doc in documents]\n",
    "    documents_text = [doc[\"content\"] for doc in documents]\n",
    "    \n",
    "    # Set batch size within ChromaDB's limits\n",
    "    batch_size = 5000\n",
    "    \n",
    "    # Add documents in batches\n",
    "    total_docs = len(ids)\n",
    "    for i in tqdm(range(0, total_docs, batch_size)):\n",
    "        end_idx = min(i + batch_size, total_docs)\n",
    "        \n",
    "        # Create batch\n",
    "        batch_ids = ids[i:end_idx]\n",
    "        batch_embeddings = embeddings[i:end_idx]\n",
    "        batch_metadatas = metadatas[i:end_idx]\n",
    "        batch_documents = documents_text[i:end_idx]\n",
    "        \n",
    "        # Add batch to collection\n",
    "        collection.add(\n",
    "            ids=batch_ids,\n",
    "            embeddings=batch_embeddings,\n",
    "            metadatas=batch_metadatas,\n",
    "            documents=batch_documents\n",
    "        )\n",
    "    \n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_similar_packets(query, collection, n=5):\n",
    "    # Load the same model used for document encoding\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Convert query to embedding\n",
    "    query_embedding = model.encode(query).tolist()\n",
    "    \n",
    "    # Query the collection\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n\n",
    "    )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITE_DB = False\n",
    "\n",
    "if OVERWRITE_DB:\n",
    "    client.reset()\n",
    "\n",
    "test_size = 100\n",
    "train_size = len(data) - test_size\n",
    "\n",
    "if os.path.exists(vector_db_path) and not OVERWRITE_DB:\n",
    "    collection = client.get_collection(collection_name)\n",
    "    if use_luflow:\n",
    "        test_data = data.iloc[train_size:]\n",
    "    else:\n",
    "        test_data = data.iloc[train_size:].drop(columns=[\"Label\", \"Dataset\"])\n",
    "    \n",
    "else:\n",
    "    # Convert to documents\n",
    "    if use_luflow:\n",
    "        train_data = data.iloc[:train_size].drop(columns=[\"label\"])\n",
    "        test_data = data.iloc[train_size:]\n",
    "    else:\n",
    "        train_data = data.iloc[:train_size].drop(columns=[\"Label\", \"Dataset\"])\n",
    "        test_data = data.iloc[train_size:].drop(columns=[\"Label\", \"Dataset\"])\n",
    "    \n",
    "    train_documents = prepare_packet_documents_luflow(train_data) if use_luflow else prepare_packet_documents_uq(train_data)\n",
    "    # Create embeddings\n",
    "    embedded_docs = create_embeddings(train_documents)\n",
    "    collection = store_in_vector_db(embedded_docs, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_documents = prepare_packet_documents_luflow(test_data) if use_luflow else prepare_packet_documents_uq(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Flow ID: 1062102\\nConnection: 192.168.1.32:52904 → 192.168.35.76:443\\nProtocol: 6 (L7: 91.0)\\nTraffic Volume: 44 bytes total (1 packets)\\nInbound: 44 bytes in 1 packets\\nOutbound: 0 bytes in 0 packets\\nTCP Flags: 2\\nClient TCP Flags: 2\\nServer TCP Flags: 0\\nTCP Window Max (In/Out): 1024/0\\nRetransmissions: 0 packets (0 bytes)\\nDNS Query ID: 0\\nDNS Query Type: 0\\nDNS TTL Answer: 0\\nFTP Return Code: 0.0\\nFlow Duration: 0 ms\\nDuration In/Out: 0/0\\nThroughput Client→Server: 352000 bytes/s\\nThroughput Server→Client: 0 bytes/s\\nPacket Size Distribution:\\n  ≤128 bytes: 1 packets\\n  128-256 bytes: 0 packets\\n  256-512 bytes: 0 packets\\n  512-1024 bytes: 0 packets\\n  1024-1514 bytes: 0 packets\\nTTL Range: 0-0\\nPacket Length Range: 0-44\\nShortest/Longest Packet: 44/44\\nAttack: <unknown>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_doc = test_documents[0].copy()\n",
    "example_doc['content'] = re.sub(r'Attack: .+', 'Attack: <unknown>', example_doc['content'])\n",
    "example_doc['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mitm', 5)]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = retrieve_similar_packets(example_doc['content'], collection, n=5)\n",
    "metadata = results['metadatas']\n",
    "attacks = [match_dict['Attack'] for match_dict in metadata[0]]\n",
    "attack_counts = Counter(attacks)\n",
    "attack_counts.most_common(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scanning'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_doc['metadata']['Attack']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [00:15<02:20,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: DDoS, Predicted: injection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:17<01:19,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: Reconnaissance, Predicted: scanning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [00:19<01:16,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: ransomware, Predicted: Backdoor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [00:20<01:09,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: Reconnaissance, Predicted: scanning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [00:35<00:45,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: xss, Predicted: injection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [00:38<00:42,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: DDoS, Predicted: injection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [00:39<00:45,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: Reconnaissance, Predicted: DoS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [00:42<00:38,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: ransomware, Predicted: Backdoor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [00:42<00:36,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: Reconnaissance, Predicted: scanning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [00:56<00:37,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: password, Predicted: injection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [00:57<00:35,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: DDoS, Predicted: injection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [00:57<00:33,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: xss, Predicted: injection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63/100 [00:58<00:31,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: xss, Predicted: injection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65/100 [00:59<00:26,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: xss, Predicted: DoS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [01:03<00:22,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: DDoS, Predicted: Generic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 77/100 [01:08<00:17,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: xss, Predicted: injection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 79/100 [01:09<00:14,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: Backdoor, Predicted: Reconnaissance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [01:13<00:16,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: DDoS, Predicted: Generic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 87/100 [01:19<00:14,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: ransomware, Predicted: Backdoor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 94/100 [01:24<00:04,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: DDoS, Predicted: Generic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98/100 [01:28<00:01,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: xss, Predicted: injection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 99/100 [01:28<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: Reconnaissance, Predicted: DoS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:29<00:00,  1.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.78"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_vector_db(test_docs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for doc in tqdm(test_docs):\n",
    "        # Query the collection\n",
    "        doc_copy = doc.copy()\n",
    "        doc_copy['content'] = re.sub(r'Attack: .+', 'Attack: <unknown>', doc_copy['content'])\n",
    "        results = retrieve_similar_packets(doc_copy['content'], collection, n=5)\n",
    "        metadata = results['metadatas']\n",
    "        attacks = [match_dict['Attack'] for match_dict in metadata[0]]\n",
    "        attack_counts = Counter(attacks)\n",
    "        most_common_attack = attack_counts.most_common(1)[0][0]\n",
    "        \n",
    "        if most_common_attack == doc['metadata']['Attack']:\n",
    "            correct += 1\n",
    "        else:\n",
    "            print(f\"Expected: {doc['metadata']['Attack']}, Predicted: {most_common_attack}\")\n",
    "        total += 1\n",
    "    \n",
    "    return correct / total\n",
    "    \n",
    "test_accuracy = validate_vector_db(test_documents)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_filters(collection, query_text, metadata_filters, n_results=5):\n",
    "    \"\"\"Query with both semantic search and metadata filters.\"\"\"\n",
    "    \n",
    "    # Load embedding model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Encode the query\n",
    "    query_embedding = model.encode(query_text).tolist()\n",
    "    \n",
    "    # Convert flat filters to proper ChromaDB format\n",
    "    # ChromaDB requires a single operator at the top level\n",
    "    formatted_where = {\"$and\": []}\n",
    "    \n",
    "    for key, value in metadata_filters.items():\n",
    "        if isinstance(value, dict):\n",
    "            # This is already an operator (like {\"$gt\": 4.0})\n",
    "            formatted_where[\"$and\"].append({key: value})\n",
    "        else:\n",
    "            # This is a direct value match\n",
    "            formatted_where[\"$and\"].append({key: {\"$eq\": value}})\n",
    "    \n",
    "    # Search the collection with properly formatted filters\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        where=formatted_where,\n",
    "        n_results=n_results\n",
    "    )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [02:25<00:00, 72.53s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.68s/it]\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"ehristoforu/coolqwen-3b-it\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Create the text-generation pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "# Function to retrieve similar network packets for LLM context\n",
    "def retrieve_context_for_llm(query_packet, collection, n=5):\n",
    "    results = retrieve_similar_packets(query_packet, collection, n=n)\n",
    "    context = \"SIMILAR NETWORK TRAFFIC EXAMPLES:\\n\\n\"\n",
    "    if 'documents' in results and results['documents'][0]:\n",
    "        for i, doc in enumerate(results['documents'][0]):\n",
    "            distance = results['distances'][0][i] if 'distances' in results else \"unknown\"\n",
    "            context += f\"--- Example {i+1} (Similarity: {1/(1+distance):.2f}) ---\\n\"\n",
    "            context += doc + \"\\n\\n\"\n",
    "    return context\n",
    "\n",
    "# Function to query an LLM using Hugging Face Transformers\n",
    "def query_llm(prompt, model, max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = model(prompt, return_full_text=False)\n",
    "            return response[0]['generated_text']\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                sleep_time = 2 ** attempt\n",
    "                print(f\"Error querying LLM: {e}. Retrying in {sleep_time}s...\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                print(f\"Failed to query LLM after {max_retries} attempts: {e}\")\n",
    "                return \"{}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nids_rag_analysis(packet_content, collection, model):\n",
    "    \"\"\"\n",
    "    Perform RAG-based network intrusion detection\n",
    "    \n",
    "    Args:\n",
    "        packet_content: The formatted packet content to analyze\n",
    "        collection: Vector database collection containing network traffic examples\n",
    "        llm_client: Client for your LLM (e.g., OpenAI, Anthropic, etc.)\n",
    "        model: SentenceTransformer model for embedding\n",
    "    \n",
    "    Returns:\n",
    "        Dict with analysis results\n",
    "    \"\"\"\n",
    "    # Retrieve similar examples as context\n",
    "    context = retrieve_context_for_llm(packet_content, collection)\n",
    "    \n",
    "    # Construct prompt for the LLM\n",
    "    prompt = f\"\"\"You are a network security expert analyzing network traffic for potential intrusions.\n",
    "    \n",
    "I'll provide you with:\n",
    "1. A network packet to analyze\n",
    "2. Examples of similar network traffic from our database with known classifications\n",
    "\n",
    "Based on the examples and your expertise, determine:\n",
    "- If this packet is malicious or benign\n",
    "- If malicious, what type of attack it represents (e.g., DoS, Port Scan, DDoS, etc.)\n",
    "- Confidence level (0-1)\n",
    "- Explanation of your reasoning\n",
    "\n",
    "Here is the packet to analyze:\n",
    "\n",
    "{packet_content}\n",
    "\n",
    "Here are similar packets from our database for context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Provide your analysis in JSON format with the following fields:\n",
    "{{\n",
    "  \"is_malicious\": true/false,\n",
    "  \"attack_type\": \"attack name or null if benign\",\n",
    "  \"confidence\": 0.xx,\n",
    "  \"explanation\": \"your detailed reasoning\"\n",
    "}}\n",
    "\n",
    "Return only valid JSON, with no additional text.\n",
    "\"\"\"\n",
    "    \n",
    "    # Send to LLM and get response\n",
    "    response = query_llm(prompt, model)\n",
    "    # remove newlines from response\n",
    "    response = response.replace(\"\\n\", \"\")\n",
    "\n",
    "    # Extract JSON from the response using regex\n",
    "    json_matches = re.findall(r'\\{.*?\\}', response, re.DOTALL)\n",
    "    \n",
    "    if json_matches:\n",
    "        json_str = json_matches[0]\n",
    "        try:\n",
    "            analysis = json.loads(json_str)\n",
    "            return analysis\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    # Parse the JSON response\n",
    "    try:\n",
    "        analysis = json.loads(response)\n",
    "        return analysis\n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback if LLM doesn't return valid JSON\n",
    "        return {\n",
    "            \"is_malicious\": None,\n",
    "            \"attack_type\": None,\n",
    "            \"confidence\": 0,\n",
    "            \"explanation\": \"Error parsing LLM response\",\n",
    "            \"raw_response\": response\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_malicious': True,\n",
       " 'attack_type': 'scanning',\n",
       " 'confidence': 0.99,\n",
       " 'explanation': 'The packet has a high similarity score (0.99) with other benign scanning packets. It fits the pattern of a scanning attack, where the client initiates a TCP connection without establishing a session, sending small packets over a wide range of ports to identify open services on the target host.'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nids_rag_analysis(example_doc['content'], collection, pipe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
