{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d33123a1",
   "metadata": {},
   "source": [
    "Insight 1: In contrast to the scarcity of global optima, local optima are commonly prevalent and perform well, making them more valuable for query-efficient prompt optimization. \\\n",
    "Insight 2:  the selection of the input domain, including both the generation and representation of prompt candidates, will influence the identification of high-performing prompts     especially those local optimal ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b979ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riley/base/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Callable\n",
    "from functools import partial\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score\n",
    "import random\n",
    "import os\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cded9410",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZOPOAlgorithm:\n",
    "    def __init__(\n",
    "        self,\n",
    "        prompt_generation: Callable,\n",
    "        nlp_embedding: Callable,\n",
    "        inverse_embedding: Callable,\n",
    "        projection_function: Callable,\n",
    "        evaluation_function: Callable,\n",
    "        m: int = 10,\n",
    "        T: int = 5,\n",
    "        delta: float = 0.1,  # δ parameter from Prop. 1\n",
    "        kappa: float = 1.0,  # κ parameter bounding kernel function\n",
    "        alpha: float = 0.01,  # Step size for gradient updates\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the ZOPO (Zero-Order Prompt Optimization) Algorithm\n",
    "        \n",
    "        Args:\n",
    "            prompt_generation_model: Model g(·) that generates prompts\n",
    "            nlp_embedding_model: Model h(·) that embeds prompts into vector space\n",
    "            inverse_embedding_model: Function h^-1(·) that maps from embedding back to prompt\n",
    "            projection_function: Function P_Z for projection in the embedding space\n",
    "            evaluation_function: Function F(·) to evaluate embeddings\n",
    "            m: Size of prompt candidates\n",
    "            T: Number of iterations\n",
    "            delta: Confidence parameter δ ∈ (0, 1)\n",
    "            kappa: Upper bound κ on kernel function\n",
    "            alpha: Step size for gradient updates\n",
    "        \"\"\"\n",
    "        self.g = prompt_generation\n",
    "        self.h = nlp_embedding\n",
    "        self.h_inverse = inverse_embedding\n",
    "        self.P_Z = projection_function\n",
    "        self.F = evaluation_function\n",
    "        self.m = m\n",
    "        self.T = T\n",
    "        self.delta = delta\n",
    "        self.kappa = kappa\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.generator = pipeline('text-generation', model='ehristoforu/coolqwen-3b-it', device_map='auto')\n",
    "        model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        self.embedding_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.embedding_model = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # For tracking historical queries and their relevance\n",
    "        self.historical_queries = {}  # Dictionary mapping z to its historical queries\n",
    "        self.query_evaluations = {}   # Store F(z) values for all queries\n",
    "        \n",
    "    def run(self, D_demo):\n",
    "        \"\"\"\n",
    "        Run the ZOPO algorithm\n",
    "        \n",
    "        Args:\n",
    "            D_demo: Demonstration data\n",
    "            \n",
    "        Returns:\n",
    "            Optimized prompt\n",
    "        \"\"\"\n",
    "        # Initialize sets V and Z\n",
    "        V = set()  # Set of prompts\n",
    "        Z = set()  # Set of embeddings\n",
    "        \n",
    "        # Step 2-6: Fill V and Z with m prompt candidates\n",
    "        while len(V) < self.m:\n",
    "            # Generate a prompt using model g\n",
    "            v = self.g(self.generator, D_demo)\n",
    "            \n",
    "            # Embed the prompt using model h\n",
    "            z = self.h(v, self.embedding_model, self.embedding_tokenizer)\n",
    "            \n",
    "            # Convert z to tuple for hashability if it's a numpy array\n",
    "            if isinstance(z, np.ndarray):\n",
    "                z_key = tuple(z.flatten())\n",
    "            else:\n",
    "                z_key = z\n",
    "            \n",
    "            # Add to sets if not already present\n",
    "            if v not in V:\n",
    "                V.add(v)\n",
    "                Z.add(z_key)\n",
    "                \n",
    "                # Initialize historical queries for this z\n",
    "                self.historical_queries[z_key] = []\n",
    "                \n",
    "                # Evaluate F(z) and store it\n",
    "                self.query_evaluations[z_key] = self.F(z)\n",
    "        \n",
    "        # Convert Z to a list for iteration and indexing\n",
    "        Z_list = list(Z)\n",
    "        \n",
    "        # Step 7-12: Iterate T times\n",
    "        for t in range(1, self.T + 1):\n",
    "            for i, z_t in enumerate(Z_list):\n",
    "                # Check if local exploration is needed based on uncertainty\n",
    "                if self.should_explore(z_t):\n",
    "                    # Perform uncertainty-informed local exploration\n",
    "                    eta_t = self.uncertainty_informed_local_exploration(z_t)\n",
    "                    \n",
    "                    # Update z_t+1 using projection\n",
    "                    # Apply gradient step: z + η where η is the exploration direction\n",
    "                    z_t_plus_eta = self.add_vectors(z_t, eta_t)\n",
    "                    z_t_plus_1 = self.P_Z(z_t_plus_eta)\n",
    "                    \n",
    "                    # Map back to prompt space\n",
    "                    v_t_plus_1 = self.h_inverse(z_t_plus_1)\n",
    "                    \n",
    "                    # Convert to hashable format\n",
    "                    if isinstance(z_t_plus_1, np.ndarray):\n",
    "                        z_t_plus_1_key = tuple(z_t_plus_1.flatten())\n",
    "                    else:\n",
    "                        z_t_plus_1_key = z_t_plus_1\n",
    "                    \n",
    "                    # Query to yield evaluation\n",
    "                    F_z_t_plus_1 = self.F(z_t_plus_1)\n",
    "                    \n",
    "                    # Store evaluation and update historical queries\n",
    "                    self.query_evaluations[z_t_plus_1_key] = F_z_t_plus_1\n",
    "                    \n",
    "                    # Add this query to historical queries for z_t\n",
    "                    self.historical_queries[z_t].append(z_t_plus_1_key)\n",
    "                    \n",
    "                    # Initialize historical queries for the new point if needed\n",
    "                    if z_t_plus_1_key not in self.historical_queries:\n",
    "                        self.historical_queries[z_t_plus_1_key] = []\n",
    "                    \n",
    "                    # Update the list with the new point\n",
    "                    Z_list[i] = z_t_plus_1_key\n",
    "        \n",
    "        # Step 13-14: Return the best prompt\n",
    "        z_star = max(Z_list, key=lambda z: self.query_evaluations[z])\n",
    "        v_star = self.h_inverse(z_star)\n",
    "        \n",
    "        return v_star\n",
    "    \n",
    "    def add_vectors(self, z, eta):\n",
    "        \"\"\"Helper function to add vectors that might be in tuple form\"\"\"\n",
    "        if isinstance(z, tuple) and isinstance(eta, tuple):\n",
    "            return tuple(np.array(z) + np.array(eta))\n",
    "        elif isinstance(z, tuple):\n",
    "            return tuple(np.array(z) + eta)\n",
    "        elif isinstance(eta, tuple):\n",
    "            return tuple(z + np.array(eta))\n",
    "        else:\n",
    "            return z + eta\n",
    "    \n",
    "    def should_explore(self, z):\n",
    "        \"\"\"\n",
    "        Determine if we should do local exploration for embedding z\n",
    "        According to Section 4.3, this is defined by the indicator function I_A_t(z_t)\n",
    "        which equals 1 if z_t ∈ A_t and 0 otherwise.\n",
    "        \n",
    "        Args:\n",
    "            z: The embedding to check\n",
    "            \n",
    "        Returns:\n",
    "            Boolean indicating whether to explore\n",
    "        \"\"\"\n",
    "        # Calculate the effective relevance β for z based on historical queries\n",
    "        beta = self.calculate_effective_relevance(z)\n",
    "        \n",
    "        # Get the number of relevant queries |N_{z,β}|\n",
    "        N_z_beta = self.get_relevant_queries_count(z, beta)\n",
    "        \n",
    "        # According to the paper, we should explore when β or |N_{z,β}| is small\n",
    "        # Let's define thresholds for these values\n",
    "        min_beta_threshold = 0.3  # Minimum acceptable effective relevance\n",
    "        min_queries_threshold = 5  # Minimum number of relevant queries\n",
    "        \n",
    "        # Exploration condition: z_t ∈ A_t\n",
    "        return beta < min_beta_threshold or N_z_beta < min_queries_threshold\n",
    "    \n",
    "    def calculate_effective_relevance(self, z):\n",
    "        \"\"\"\n",
    "        Calculate effective relevance β for a given input z\n",
    "        This is a measure of how relevant historical queries are for gradient estimation at z\n",
    "        \n",
    "        Args:\n",
    "            z: The input embedding\n",
    "            \n",
    "        Returns:\n",
    "            The effective relevance β\n",
    "        \"\"\"\n",
    "        # This would depend on the specific kernel function k(z, z')\n",
    "        # and the gradient estimation method being used\n",
    "        # For simplicity, we'll use a placeholder implementation\n",
    "        historical_queries = self.historical_queries.get(z, [])\n",
    "        \n",
    "        if not historical_queries:\n",
    "            return 0.0  # No historical queries, so relevance is zero\n",
    "        \n",
    "        # Calculate kernel-based similarity between z and historical queries\n",
    "        similarities = [self.kernel(z, z_prime) for z_prime in historical_queries]\n",
    "        \n",
    "        # Beta is the average similarity\n",
    "        return sum(similarities) / len(similarities) if similarities else 0.0\n",
    "    \n",
    "    def kernel(self, z, z_prime):\n",
    "        \"\"\"\n",
    "        Kernel function k(z, z') measuring similarity between embeddings\n",
    "        The paper assumes k(z, z') ≤ α and |k''(z, z)| ≤ κ\n",
    "        \n",
    "        Args:\n",
    "            z: First embedding\n",
    "            z_prime: Second embedding\n",
    "            \n",
    "        Returns:\n",
    "            Kernel similarity value\n",
    "        \"\"\"\n",
    "        # Convert tuples to arrays if needed\n",
    "        z_array = np.array(z) if isinstance(z, tuple) else z\n",
    "        z_prime_array = np.array(z_prime) if isinstance(z_prime, tuple) else z_prime\n",
    "        \n",
    "        # Implementation of a simple RBF kernel\n",
    "        # k(z, z') = exp(-γ||z - z'||²)\n",
    "        gamma = 0.1  # Width parameter\n",
    "        distance_squared = np.sum((z_array - z_prime_array) ** 2)\n",
    "        return np.exp(-gamma * distance_squared)\n",
    "    \n",
    "    def get_relevant_queries_count(self, z, beta):\n",
    "        \"\"\"\n",
    "        Get the count of relevant queries N_{z,β} for a given input z and relevance threshold β\n",
    "        N_{z,β} = {z' ∈ {z_τ}_{τ=1}^t : ||∂_z k(z', z)||² ≥ β}\n",
    "        \n",
    "        Args:\n",
    "            z: The input embedding\n",
    "            beta: The effective relevance threshold\n",
    "            \n",
    "        Returns:\n",
    "            The count of relevant queries\n",
    "        \"\"\"\n",
    "        historical_queries = self.historical_queries.get(z, [])\n",
    "        \n",
    "        # Count queries where the gradient of the kernel exceeds β\n",
    "        relevant_count = 0\n",
    "        for z_prime in historical_queries:\n",
    "            gradient_norm_squared = self.kernel_gradient_norm_squared(z_prime, z)\n",
    "            if gradient_norm_squared >= beta:\n",
    "                relevant_count += 1\n",
    "                \n",
    "        return relevant_count\n",
    "    \n",
    "    def kernel_gradient_norm_squared(self, z_prime, z):\n",
    "        \"\"\"\n",
    "        Calculate ||∂_z k(z', z)||² - the squared norm of the gradient of the kernel\n",
    "        \n",
    "        Args:\n",
    "            z_prime: The first input embedding\n",
    "            z: The second input embedding with respect to which we differentiate\n",
    "            \n",
    "        Returns:\n",
    "            The squared norm of the kernel gradient\n",
    "        \"\"\"\n",
    "        # For an RBF kernel k(z, z') = exp(-γ||z - z'||²)\n",
    "        # ∂_z k(z', z) = 2γ(z' - z)exp(-γ||z - z'||²)\n",
    "        \n",
    "        # Convert tuples to arrays if needed\n",
    "        z_array = np.array(z) if isinstance(z, tuple) else z\n",
    "        z_prime_array = np.array(z_prime) if isinstance(z_prime, tuple) else z_prime\n",
    "        \n",
    "        gamma = 0.1  # Same as in kernel function\n",
    "        diff = z_prime_array - z_array\n",
    "        distance_squared = np.sum(diff ** 2)\n",
    "        k_value = np.exp(-gamma * distance_squared)\n",
    "        \n",
    "        # ||∂_z k(z', z)||² = 4γ²||z' - z||²exp(-2γ||z - z'||²)\n",
    "        gradient_norm_squared = 4 * gamma**2 * distance_squared * k_value**2\n",
    "        \n",
    "        return gradient_norm_squared\n",
    "    \n",
    "    def uncertainty_informed_local_exploration(self, z):\n",
    "        \"\"\"\n",
    "        Perform uncertainty-informed local exploration to reduce gradient estimation error\n",
    "        \n",
    "        Args:\n",
    "            z: The input embedding\n",
    "            \n",
    "        Returns:\n",
    "            Exploration direction η_t\n",
    "        \"\"\"\n",
    "        # Calculate variance of gradient estimator Σ²_t(z)\n",
    "        # This is related to predictive uncertainty\n",
    "        gradient_variance = self.calculate_gradient_variance(z)\n",
    "        \n",
    "        # Generate random direction\n",
    "        z_dim = len(np.array(z) if isinstance(z, tuple) else z)\n",
    "        random_direction = np.random.randn(z_dim)\n",
    "        random_direction = random_direction / np.linalg.norm(random_direction)\n",
    "        \n",
    "        # Scale by a factor related to the gradient variance\n",
    "        # Higher variance should lead to larger exploration steps\n",
    "        scale_factor = np.sqrt(np.trace(gradient_variance)) * self.alpha\n",
    "        \n",
    "        # Return the exploration direction η_t\n",
    "        return tuple(scale_factor * random_direction)\n",
    "    \n",
    "    def calculate_gradient_variance(self, z):\n",
    "        \"\"\"\n",
    "        Calculate the variance of the gradient estimator Σ²_t(z)\n",
    "        \n",
    "        Args:\n",
    "            z: The input embedding\n",
    "            \n",
    "        Returns:\n",
    "            Covariance matrix of gradient estimate\n",
    "        \"\"\"\n",
    "        # This would be a complex calculation based on the NTK-GP model\n",
    "        # Simplification: just return an identity matrix scaled by some uncertainty measure\n",
    "        z_dim = len(np.array(z) if isinstance(z, tuple) else z)\n",
    "        \n",
    "        # Get effective relevance and number of relevant queries\n",
    "        beta = self.calculate_effective_relevance(z)\n",
    "        N_z_beta = self.get_relevant_queries_count(z, beta)\n",
    "        \n",
    "        # According to Proposition 1, variance is bounded by ωκ - ωβ/d / (α + σ²/|N_{z,β}|)\n",
    "        # where ω = d + 2(√d + 1)ln(1/δ)\n",
    "        d = z_dim  # Dimension of the embedding space\n",
    "        delta = self.delta\n",
    "        omega = d + 2 * (np.sqrt(d) + 1) * np.log(1/delta)\n",
    "        \n",
    "        # Simplified variance estimate\n",
    "        if N_z_beta > 0:\n",
    "            variance_scale = omega * self.kappa - (omega * beta / d) / (1 + 1 / N_z_beta)\n",
    "        else:\n",
    "            variance_scale = omega * self.kappa  # Maximum uncertainty when no relevant queries\n",
    "            \n",
    "        return np.eye(z_dim) * max(variance_scale, 0.01)  # Ensure positive variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1201c6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_generation(generator, demo_data, task_description=None):\n",
    "    \"\"\"\n",
    "    Uses an LLM from Hugging Face Transformers to generate prompt candidates based on demonstration data\n",
    "    \n",
    "    Args:\n",
    "        demo_data: Example data for the task\n",
    "        task_description: Optional description of the target task\n",
    "        \n",
    "    Returns:\n",
    "        A prompt string\n",
    "    \"\"\"\n",
    "\n",
    "    # Default task if none provided\n",
    "    if task_description is None:\n",
    "        task_description = \"sentiment analysis\"\n",
    "    \n",
    "    # Format the examples into a string\n",
    "    examples_str = \"\\n\\n\".join([f\"Example {i+1}: {example}\" for i, example in enumerate(demo_data)])\n",
    "    \n",
    "    # Create a system message that instructs the LLM\n",
    "    system_message = f\"\"\"\n",
    "    You are an expert at creating effective prompts for language models. \n",
    "    Your task is to generate a concise, clear instruction prompt for {task_description}.\n",
    "    \n",
    "    The prompt should:\n",
    "    1. Be concise but clear\n",
    "    2. Provide specific instructions about the format of the expected output\n",
    "    3. Be effective at getting language models to perform the target task\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a user message with the task and examples\n",
    "    user_message = f\"\"\"\n",
    "    Task: {task_description}\n",
    "    \n",
    "    Here are some examples of the task:\n",
    "    {examples_str}\n",
    "    \n",
    "    Please generate a single effective instruction prompt for this task. Do not include any explanations, \n",
    "    just provide the prompt itself. The prompt should work well when prepended to new examples.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Format input for the model (adjust based on the specific model's format requirements)\n",
    "        # For chat models like Llama-2, this is a common format\n",
    "        input_text = f\"<s>[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\n{user_message} [/INST]\"\n",
    "        \n",
    "        # Generate the prompt\n",
    "        outputs = generator(\n",
    "            input_text,\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            return_full_text=False\n",
    "        )\n",
    "        \n",
    "        # Extract the generated prompt\n",
    "        generated_prompt = outputs[0]['generated_text'].strip()\n",
    "        return generated_prompt\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Fallback to template-based generation if model inference fails\n",
    "        print(f\"Error using transformers model: {e}\")\n",
    "        print(\"Using fallback prompt generation method\")\n",
    "        return fallback_prompt_generation(task_description)\n",
    "        \n",
    "def fallback_prompt_generation(task_description):\n",
    "    \"\"\"\n",
    "    Fallback method to generate prompts in case the model inference fails\n",
    "    \n",
    "    Args:\n",
    "        task_description: Description of the target task\n",
    "        \n",
    "    Returns:\n",
    "        A prompt string\n",
    "    \"\"\"\n",
    "    # Templates for different common tasks\n",
    "    templates = {\n",
    "        \"sentiment analysis\": [\n",
    "            \"Classify the sentiment of the following text as positive or negative:\",\n",
    "            \"Determine if the following text expresses a positive or negative sentiment:\",\n",
    "            \"Is the sentiment of the following text positive or negative?\",\n",
    "            \"Analyze the sentiment of the text below and respond with only 'positive' or 'negative':\",\n",
    "        ],\n",
    "        \"text classification\": [\n",
    "            \"Classify the following text into one of these categories: {categories}\",\n",
    "            \"Which category best describes the following text? Categories: {categories}\",\n",
    "            \"Read the following text and classify it as {categories}:\",\n",
    "        ],\n",
    "        \"summarization\": [\n",
    "            \"Summarize the following text in one paragraph:\",\n",
    "            \"Provide a concise summary of the text below:\",\n",
    "            \"Create a brief summary of the following content:\",\n",
    "        ],\n",
    "        \"question answering\": [\n",
    "            \"Answer the following question based on the given context:\",\n",
    "            \"Using the information provided, answer this question:\",\n",
    "            \"Read the context below and answer the question that follows:\",\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Determine which template set to use\n",
    "    task_key = \"sentiment analysis\"  # Default\n",
    "    for key in templates:\n",
    "        if key in task_description.lower():\n",
    "            task_key = key\n",
    "            break\n",
    "    \n",
    "    # Randomly select a template\n",
    "    template_options = templates[task_key]\n",
    "    template = np.random.choice(template_options)\n",
    "    \n",
    "    # Fill in placeholders if needed\n",
    "    if \"{categories}\" in template:\n",
    "        # Extract categories from task description or use defaults\n",
    "        if \"categories\" in task_description.lower():\n",
    "            # Try to extract categories from the task description\n",
    "            try:\n",
    "                categories_part = task_description.lower().split(\"categories:\")[1].strip()\n",
    "                categories = categories_part.split(\",\")\n",
    "                categories_str = \", \".join(categories)\n",
    "            except:\n",
    "                categories_str = \"positive, negative, neutral\"\n",
    "        else:\n",
    "            categories_str = \"positive, negative, neutral\"\n",
    "        \n",
    "        template = template.replace(\"{categories}\", categories_str)\n",
    "    \n",
    "    return template\n",
    "\n",
    "def nlp_embedding(prompt, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Creates embeddings using a pre-trained transformer model from Hugging Face.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt string to embed\n",
    "        model: Hugging Face model to use (default: sentence-transformers/all-MiniLM-L6-v2)\n",
    "        tokenizer: Tokenizer for the model\n",
    "        \n",
    "    Returns:\n",
    "        A vector representation (embedding) of the prompt\n",
    "    \"\"\"\n",
    "    # Prepare inputs\n",
    "    inputs = tokenizer(prompt, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # Use the [CLS] token embedding or mean pooling\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :].numpy()  # [CLS] token embedding\n",
    "        \n",
    "        # Alternative: mean pooling across all tokens\n",
    "        # attention_mask = inputs['attention_mask']\n",
    "        # input_mask_expanded = attention_mask.unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()\n",
    "        # sum_embeddings = torch.sum(outputs.last_hidden_state * input_mask_expanded, 1)\n",
    "        # sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        # embeddings = (sum_embeddings / sum_mask).numpy()\n",
    "    \n",
    "    return tuple(embeddings[0].tolist())  # Convert to tuple for hashability\n",
    "\n",
    "def inverse_embedding(embedding, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Maps an embedding back to the closest matching prompt template\n",
    "    \n",
    "    Args:\n",
    "        embedding: The embedding vector (tuple or array)\n",
    "        model_name: Name of the Hugging Face model used for embedding\n",
    "        \n",
    "    Returns:\n",
    "        A prompt string that most closely matches the embedding\n",
    "    \"\"\"\n",
    "    # Convert tuple to list if needed\n",
    "    if isinstance(embedding, tuple):\n",
    "        embedding = np.array(embedding)\n",
    "    \n",
    "    # Define a set of template prompts\n",
    "    templates = [\n",
    "        \"Classify the sentiment of the following text as positive or negative:\",\n",
    "        \"Determine if the following text expresses a positive or negative sentiment:\",\n",
    "        \"Is the sentiment of the following text positive or negative?\",\n",
    "        \"Analyze the following text and determine if it has a positive or negative sentiment:\",\n",
    "        \"Rate the sentiment of the following text as either positive or negative:\"\n",
    "    ]\n",
    "    \n",
    "    # Load the same model used for the embedding\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Compute embeddings for all templates\n",
    "    template_embeddings = []\n",
    "    for template in templates:\n",
    "        inputs = tokenizer(template, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            # Use the same embedding approach as in nlp_embedding_model\n",
    "            template_emb = outputs.last_hidden_state[:, 0, :].numpy()[0]\n",
    "            template_embeddings.append(template_emb)\n",
    "    \n",
    "    # Find the template with the smallest cosine distance to the input embedding\n",
    "    distances = [cosine(embedding, template_emb) for template_emb in template_embeddings]\n",
    "    closest_index = np.argmin(distances)\n",
    "    \n",
    "    return templates[closest_index]\n",
    "\n",
    "# 4. Define the projection function P_Z\n",
    "def projection_function(z):\n",
    "    \"\"\"\n",
    "    Projects the embedding back to the valid embedding space\n",
    "    \n",
    "    Args:\n",
    "        z: The embedding vector to project\n",
    "        \n",
    "    Returns:\n",
    "        The projected embedding vector\n",
    "    \"\"\"\n",
    "    # Convert tuple to array if needed\n",
    "    if isinstance(z, tuple):\n",
    "        z = np.array(z)\n",
    "    \n",
    "    # Apply constraints to ensure embedding is valid\n",
    "    # Example: ensure all values are positive\n",
    "    z = np.maximum(z, 0)\n",
    "    \n",
    "    # Example: ensure embedding dimensions stay within reasonable bounds\n",
    "    z[0] = np.clip(z[0], 3, 15)  # Number of words between 3 and 15\n",
    "    z[1] = np.clip(z[1], 3, 10)  # Average word length between 3 and 10\n",
    "    z[2] = round(z[2])  # Question mark presence is binary (0 or 1)\n",
    "    z[3] = np.clip(round(z[3]), 0, 3)  # Between 0 and 3 imperatives\n",
    "    \n",
    "    return tuple(z)  # Convert back to tuple for hashability\n",
    "\n",
    "def evaluation_function(z, task_type=\"instruction_induction\", reference_embeddings=None, \n",
    "                        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    \"\"\"\n",
    "    Evaluates the quality of a prompt embedding based on academic performance metrics.\n",
    "    \n",
    "    Args:\n",
    "        z: The embedding vector to evaluate (tuple or array)\n",
    "        task_type: Type of task for evaluation metric selection\n",
    "        reference_embeddings: Dict of reference embeddings with their scores by task\n",
    "        model_name: Name of the Hugging Face model used for embedding\n",
    "        \n",
    "    Returns:\n",
    "        A score representing relative performance compared to optimal methods\n",
    "    \"\"\"\n",
    "    # Convert tuple to array if needed\n",
    "    if isinstance(z, tuple):\n",
    "        z = np.array(z)\n",
    "    \n",
    "    # Initialize reference embeddings if not provided\n",
    "    if reference_embeddings is None:\n",
    "        # In practice, this would be pre-computed from a benchmark dataset\n",
    "        reference_embeddings = generate_reference_embeddings(model_name)\n",
    "    \n",
    "    # Define task categories and their evaluation metrics\n",
    "    task_metrics = {\n",
    "        \"common_concept\": \"f1\",\n",
    "        \"informal_to_formal\": \"f1\",\n",
    "        \"orthography_starts_with\": \"exact_match\",\n",
    "        \"taxonomy_animal\": \"exact_match\",\n",
    "        \"synonyms\": \"set_containing\",\n",
    "        \"instruction_induction\": \"exact_match\",\n",
    "        \"arithmetic_reasoning\": \"accuracy\"\n",
    "    }\n",
    "    \n",
    "    # Simulate performance on different tasks\n",
    "    task_performances = {}\n",
    "    for task, metric in task_metrics.items():\n",
    "        # Find the similarity between our embedding and reference embeddings for this task\n",
    "        task_refs = reference_embeddings.get(task, [{\"embedding\": z, \"score\": 0.5}])\n",
    "        \n",
    "        # Calculate embedding similarity to reference solutions\n",
    "        similarities = []\n",
    "        for ref in task_refs:\n",
    "            ref_emb = ref[\"embedding\"]\n",
    "            similarity = 1 - np.linalg.norm(z - ref_emb) / np.sqrt(len(z))\n",
    "            similarities.append((similarity, ref[\"score\"]))\n",
    "        \n",
    "        # Weight reference scores by embedding similarity\n",
    "        weighted_scores = [sim * score for sim, score in similarities]\n",
    "        task_score = sum(weighted_scores) / max(sum(sim for sim, _ in similarities), 1e-10)\n",
    "        \n",
    "        # Adjust based on metric type\n",
    "        if metric == \"f1\":\n",
    "            # F1 score is already between 0-1\n",
    "            pass\n",
    "        elif metric == \"exact_match\" or metric == \"accuracy\":\n",
    "            # For exact matching metrics, we want a sharper distinction\n",
    "            task_score = task_score ** 1.5\n",
    "        elif metric == \"set_containing\":\n",
    "            # For set containment, use a different weighting\n",
    "            task_score = task_score ** 0.8\n",
    "        \n",
    "        task_performances[task] = task_score\n",
    "    \n",
    "    # Calculate performance profile according to Dolan & Moré (2002)\n",
    "    # For ρ(0): count tasks where this method achieves best performance\n",
    "    # Simulate best performances for each task\n",
    "    best_performances = {task: 0.95 + 0.05 * np.random.random() for task in task_metrics}\n",
    "    \n",
    "    # Calculate ratio to best performance for each task\n",
    "    performance_ratios = {task: score / best_performances[task] for task, score in task_performances.items()}\n",
    "    \n",
    "    # Calculate ρ(0): proportion of tasks where performance is optimal\n",
    "    rho_0 = sum(1 for ratio in performance_ratios.values() if ratio >= 0.99) / len(task_metrics)\n",
    "    \n",
    "    # Calculate ρ(0.05): proportion of tasks where performance is within 5% of optimal\n",
    "    rho_5 = sum(1 for ratio in performance_ratios.values() if ratio >= 0.95) / len(task_metrics)\n",
    "    \n",
    "    # Combine scores with weighting favoring ρ(0)\n",
    "    final_score = (0.7 * rho_0) + (0.3 * rho_5)\n",
    "    \n",
    "    # Add small controlled noise for real-world variability\n",
    "    noise = np.random.normal(0, 0.03)\n",
    "    \n",
    "    return final_score + noise\n",
    "\n",
    "def generate_reference_embeddings(model_name):\n",
    "    \"\"\"\n",
    "    Generate reference embeddings for different tasks with their scores.\n",
    "    In practice, these would come from benchmark datasets.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Example prompts by task with their effectiveness scores (would be much more extensive in practice)\n",
    "    task_prompts = {\n",
    "        \"common_concept\": [\n",
    "            {\"prompt\": \"What concept do these items share in common?\", \"score\": 0.92},\n",
    "            {\"prompt\": \"Identify the common category for these objects.\", \"score\": 0.88}\n",
    "        ],\n",
    "        \"informal_to_formal\": [\n",
    "            {\"prompt\": \"Convert this informal text to formal language.\", \"score\": 0.94},\n",
    "            {\"prompt\": \"Rewrite the following in a professional tone.\", \"score\": 0.89}\n",
    "        ],\n",
    "        \"orthography_starts_with\": [\n",
    "            {\"prompt\": \"List five words that start with the prefix:\", \"score\": 0.91},\n",
    "            {\"prompt\": \"Generate words beginning with the following letters:\", \"score\": 0.87}\n",
    "        ],\n",
    "        \"taxonomy_animal\": [\n",
    "            {\"prompt\": \"Classify this animal in the taxonomic hierarchy.\", \"score\": 0.93},\n",
    "            {\"prompt\": \"Place this organism in its correct biological classification.\", \"score\": 0.90}\n",
    "        ],\n",
    "        \"synonyms\": [\n",
    "            {\"prompt\": \"Provide synonyms for the following word:\", \"score\": 0.95},\n",
    "            {\"prompt\": \"What are alternative words with the same meaning as:\", \"score\": 0.89}\n",
    "        ],\n",
    "        \"instruction_induction\": [\n",
    "            {\"prompt\": \"Based on these examples, what's the underlying pattern?\", \"score\": 0.91},\n",
    "            {\"prompt\": \"Identify the rule being demonstrated in these instances.\", \"score\": 0.88}\n",
    "        ],\n",
    "        \"arithmetic_reasoning\": [\n",
    "            {\"prompt\": \"Solve this math problem step by step:\", \"score\": 0.94},\n",
    "            {\"prompt\": \"Calculate the answer and show your work:\", \"score\": 0.90}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Generate embeddings for each prompt\n",
    "    reference_embeddings = {}\n",
    "    for task, prompts in task_prompts.items():\n",
    "        reference_embeddings[task] = []\n",
    "        for item in prompts:\n",
    "            inputs = tokenizer(item[\"prompt\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                emb = outputs.last_hidden_state[:, 0, :].numpy()[0]\n",
    "                reference_embeddings[task].append({\"embedding\": emb, \"score\": item[\"score\"]})\n",
    "    \n",
    "    return reference_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6ac1eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.20s/it]\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_name = \"ehristoforu/coolqwen-3b-it\"\n",
    "model = pipeline('text-generation', model=model_name, device_map='auto')\n",
    "\n",
    "class GenerationTemplate:\n",
    "    \"\"\"\n",
    "    Takes a prompt template and provides methods for filling in blanks.\n",
    "    The format is as follows:\n",
    "    [APE] is where text will be generated by the LLM.\n",
    "    [full_DEMO] is where the full demo will be inserted.\n",
    "    [INPUT] is where the input to the first demo will be inserted.\n",
    "    [OUTPUT] is where the output from the first demo will be inserted.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, template):\n",
    "        self.template = template\n",
    "        # Check that the template is valid\n",
    "        # There should be exactly one [APE] token\n",
    "        assert self.template.count('[APE]') == 1\n",
    "\n",
    "    def fill(self, full_demo='', input='', output=''):\n",
    "        \"\"\"\n",
    "        Fills in the template with the given values.\n",
    "        \"\"\"\n",
    "        return self.template.replace('[full_DEMO]', full_demo).replace(\n",
    "            '[INPUT]', input).replace('[OUTPUT]', output)\n",
    "\n",
    "\n",
    "class EvalTemplate:\n",
    "    \"\"\"\n",
    "    Takes a prompt template and provides methods for filling in blanks.\n",
    "    The format is as follows:\n",
    "    [PROMPT] is where the prompt will be inserted.\n",
    "    [full_DEMO] is where the full demo will be inserted.\n",
    "    [INPUT] is where the input to the first demo will be inserted.\n",
    "    [OUTPUT] is where the output from the first demo will be inserted.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, template):\n",
    "        self.template = template\n",
    "\n",
    "    def fill(self, prompt='', full_demo='', input='', output=''):\n",
    "        \"\"\"\n",
    "        Fills in the template with the given values.\n",
    "        \"\"\"\n",
    "        return self.template.replace('[PROMPT]', prompt).replace(\n",
    "            '[full_DEMO]', full_demo).replace('[INPUT]', input).replace('[OUTPUT]', output)\n",
    "\n",
    "    def convert_to_generation_template(self):\n",
    "        \"\"\"\n",
    "        Converts the evaluation template to a generation template.\n",
    "        \"\"\"\n",
    "        return GenerationTemplate(self.template.replace('[PROMPT]', '[APE]'))\n",
    "\n",
    "\n",
    "class DemosTemplate:\n",
    "    \"\"\"\n",
    "    Takes a template for the full demo and provides methods for filling in blanks.\n",
    "    The format is as follows:\n",
    "    [INPUT], [OUTPUT]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, template, delimiter='\\n\\n'):\n",
    "        self.template = template\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def fill(self, data):\n",
    "        \"\"\"\n",
    "        Fills in the template with the given values. Data is a tuple of lists.\n",
    "        \"\"\"\n",
    "        demos = ''\n",
    "        for i, (input_, output_) in enumerate(zip(*data)):\n",
    "            demos += self.template.replace('[INPUT]', input_).replace(\n",
    "                '[OUTPUT]', output_)\n",
    "\n",
    "            if i != len(data[0]) - 1:\n",
    "                demos += self.delimiter\n",
    "\n",
    "        return demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d9b62e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationResult(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def sorted(self, method='default'):\n",
    "        \"\"\"Get the results in the form of a sorted prompt and score list.\n",
    "        Has a method argument to support sorting by various metrics.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def in_place(self, method='default'):\n",
    "        \"\"\"Get the results in the form of a list of prompts and scores without sorting.\"\"\"\n",
    "        pass\n",
    "\n",
    "class BanditsEvaluationResult(EvaluationResult):\n",
    "\n",
    "    def __init__(self, prompts, scores, infos):\n",
    "        self.prompts = prompts\n",
    "        self.scores = scores\n",
    "        self.infos = infos\n",
    "\n",
    "    def sorted(self, method='default'):\n",
    "        \"\"\"Sort the prompts and scores. There is no choice of method for now.\"\"\"\n",
    "        idx = np.argsort(self.scores)\n",
    "        prompts, scores = [self.prompts[i]\n",
    "                           for i in idx], [self.scores[i] for i in idx]\n",
    "        # Reverse\n",
    "        prompts, scores = prompts[::-1], scores[::-1]\n",
    "        return prompts, scores\n",
    "\n",
    "    def in_place(self, method='default'):\n",
    "        \"\"\"Return the prompts and scores in place. There is no choice of method for now.\"\"\"\n",
    "        return self.prompts, self.scores\n",
    "\n",
    "    def sorted_infos(self):\n",
    "        \"\"\"Sort the infos.\"\"\"\n",
    "        idx = np.argsort(self.scores)\n",
    "        infos = [self.infos[i] for i in idx]\n",
    "        # Reverse\n",
    "        infos = infos[::-1]\n",
    "        return infos\n",
    "\n",
    "    def __str__(self):\n",
    "        s = ''\n",
    "        prompts, scores = self.sorted()\n",
    "        s += 'score: prompt\\n'\n",
    "        s += '----------------\\n'\n",
    "        for prompt, score in list(zip(prompts, scores))[:10]:\n",
    "            s += f'{score:.2f}: {prompt}\\n'\n",
    "        return s\n",
    "\n",
    "\n",
    "class BatchBanditAlgo(ABC):\n",
    "\n",
    "    @ abstractmethod\n",
    "    def choose(self, n):\n",
    "        \"\"\"Choose n prompts from the scores.\n",
    "        Parameters:\n",
    "            n: The number of prompts to choose.\n",
    "        Returns:\n",
    "            A list of indices of the chosen prompts.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @ abstractmethod\n",
    "    def update(self, chosen, scores):\n",
    "        \"\"\"Update the scores for the chosen prompts.\n",
    "        Parameters:\n",
    "            chosen: A list of indices of the chosen prompts.\n",
    "            scores: A list of scores for each chosen prompt in the form of a list.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @ abstractmethod\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the algorithm.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @ abstractmethod\n",
    "    def get_scores(self):\n",
    "        \"\"\"Get the scores for all prompts.\n",
    "        Returns:\n",
    "            A list of scores.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @ abstractmethod\n",
    "    def get_infos(self):\n",
    "        \"\"\"Get the infos for all prompts.\n",
    "        Returns:\n",
    "            A list of infos.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class CountAverageBanditAlgo(BatchBanditAlgo):\n",
    "\n",
    "    def __init__(self, num_prompts, num_samples):\n",
    "        self.num_prompts = num_prompts\n",
    "        self.num_samples = num_samples\n",
    "        self.reset()\n",
    "\n",
    "    def update(self, chosen, scores):\n",
    "        for i, score in zip(chosen, scores):\n",
    "            self.counts[i] += self.num_samples\n",
    "            self.scores[i] += score * self.num_samples\n",
    "\n",
    "    def reset(self):\n",
    "        self.counts = np.zeros(self.num_prompts)\n",
    "        self.scores = np.zeros(self.num_prompts)\n",
    "\n",
    "    def get_scores(self):\n",
    "        # Some counts may be 0, so we need to avoid division by 0.\n",
    "        return np.divide(self.scores, self.counts, out=np.zeros_like(self.scores), where=self.counts != 0)\n",
    "\n",
    "\n",
    "class UCBBanditAlgo(CountAverageBanditAlgo):\n",
    "\n",
    "    def __init__(self, num_prompts, num_samples, c):\n",
    "        super().__init__(num_prompts, num_samples)\n",
    "        self.c = c\n",
    "\n",
    "    def choose(self, n):\n",
    "        if np.sum(self.counts) == 0:\n",
    "            # If all counts are 0, choose randomly.\n",
    "            return random.sample(range(self.num_prompts), n)\n",
    "        scores = self.get_scores()\n",
    "        counts = self.counts + 1e-3\n",
    "        ucb_scores = scores + self.c * np.sqrt(np.log(np.sum(counts)) / counts)\n",
    "        # Choose the prompts with the highest UCB scores\n",
    "        return np.argsort(ucb_scores)[::-1][:n]\n",
    "\n",
    "    def get_infos(self):\n",
    "        return self.counts\n",
    "\n",
    "def get_bandit_algo(bandit_method, num_prompts, config):\n",
    "    \"\"\"\n",
    "    Returns the bandit method object.\n",
    "    Parameters:\n",
    "        bandit_method: The bandit method to use. ('epsilon-greedy')\n",
    "    Returns:\n",
    "        A bandit method object.\n",
    "    \"\"\"\n",
    "    if bandit_method == 'ucb':\n",
    "        return UCBBanditAlgo(num_prompts, config['base_eval_config']['num_samples'], config['bandit_config']['c'])\n",
    "    else:\n",
    "        raise ValueError('Invalid bandit method.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c6c9e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandits_evaluator(prompts, eval_template, eval_data, demos_template, few_shot_data, config):\n",
    "    base_eval_method = get_eval_method(config['base_eval_method'])\n",
    "    bandit_algo = get_bandit_algo(\n",
    "        config['bandit_method'], len(prompts), config)\n",
    "    rounds = config['rounds']\n",
    "    if config['num_prompts_per_round'] < 1:\n",
    "        num_prompts_per_round = int(\n",
    "            len(prompts) * config['num_prompts_per_round'])\n",
    "    else:\n",
    "        num_prompts_per_round = config['num_prompts_per_round']\n",
    "    num_prompts_per_round = min(num_prompts_per_round, len(prompts))\n",
    "    for _ in tqdm(range(rounds), desc='Evaluating prompts'):\n",
    "        # Sample the prompts\n",
    "        sampled_prompts_idx = bandit_algo.choose(num_prompts_per_round)\n",
    "        sampled_prompts = [prompts[i] for i in sampled_prompts_idx]\n",
    "        # Evaluate the sampled prompts\n",
    "        sampled_eval_results = base_eval_method(\n",
    "            sampled_prompts, eval_template, eval_data, demos_template, few_shot_data, config['base_eval_config'])\n",
    "        _, scores = sampled_eval_results.in_place(method='mean')\n",
    "        # Update the bandit algorithm\n",
    "        bandit_algo.update(sampled_prompts_idx, scores)\n",
    "\n",
    "    return BanditsEvaluationResult(prompts, bandit_algo.get_scores(), bandit_algo.get_infos())\n",
    "\n",
    "def subsample_data(data, subsample_size):\n",
    "    \"\"\"\n",
    "    Subsample data. Data is in the form of a tuple of lists.\n",
    "    \"\"\"\n",
    "    inputs, outputs = data\n",
    "    assert len(inputs) == len(outputs)\n",
    "    indices = random.sample(range(len(inputs)), subsample_size)\n",
    "    inputs = [inputs[i] for i in indices]\n",
    "    outputs = [outputs[i] for i in indices]\n",
    "    return inputs, outputs\n",
    "\n",
    "class LikelihoodEvaluationResult(EvaluationResult):\n",
    "    \"\"\"\n",
    "    A class for storing the results of a likelihood evaluation. Supports\n",
    "    sorting prompts by various statistics of the likelihoods.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, prompts, log_probs, num_samples):\n",
    "        self.prompts = prompts\n",
    "        self.log_probs = log_probs\n",
    "        self.prompt_log_probs = self._compute_avg_likelihood(\n",
    "            prompts, log_probs, num_samples)\n",
    "\n",
    "    def _compute_avg_likelihood(self, prompts, log_probs, num_samples):\n",
    "        i = 0\n",
    "        prompt_log_probs = []\n",
    "        for prompt in prompts:\n",
    "            prompt_log_probs.append([])\n",
    "            for _ in range(num_samples):\n",
    "                lps = log_probs[i]\n",
    "                prompt_log_probs[-1].append(sum(lps) / len(lps))\n",
    "                i += 1\n",
    "        return prompt_log_probs\n",
    "\n",
    "    def _agg_likelihoods(self, method):\n",
    "        \"\"\"For each prompt, compute a statistic of the likelihoods (e.g., mean, median, etc.)\"\"\"\n",
    "        if method == 'mean':\n",
    "            return [np.mean(lps) for lps in self.prompt_log_probs]\n",
    "        elif method == 'median':\n",
    "            return [np.median(lps) for lps in self.prompt_log_probs]\n",
    "        elif method == 'std':\n",
    "            return [np.std(lps) for lps in self.prompt_log_probs]\n",
    "        elif method == 'max':\n",
    "            return [np.max(lps) for lps in self.prompt_log_probs]\n",
    "        elif method == 'min':\n",
    "            return [np.min(lps) for lps in self.prompt_log_probs]\n",
    "        elif method == 'iqm':\n",
    "            return [np.mean(np.percentile(lps, [25, 75])) for lps in self.prompt_log_probs]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'Unknown method {method} for aggregating likelihoods')\n",
    "\n",
    "    def sorted(self, method='default'):\n",
    "        if method == 'default':\n",
    "            scores = self._agg_likelihoods('mean')\n",
    "        else:\n",
    "            scores = self._agg_likelihoods(method)\n",
    "        # Sort prompts by score\n",
    "        sorted_prompts = [p for _, p in sorted(zip(scores, self.prompts))]\n",
    "        sorted_scores = sorted(scores)\n",
    "        # Reverse both and convert to lists\n",
    "        sorted_prompts = list(reversed(sorted_prompts))\n",
    "        sorted_scores = list(reversed(sorted_scores))\n",
    "        return sorted_prompts, sorted_scores\n",
    "\n",
    "    def in_place(self, method='default'):\n",
    "        if method == 'default':\n",
    "            scores = self._agg_likelihoods('mean')\n",
    "        else:\n",
    "            scores = self._agg_likelihoods(method)\n",
    "        return self.prompts, scores\n",
    "\n",
    "    def __str__(self):\n",
    "        s = ''\n",
    "        prompts, scores = self.sorted()\n",
    "        s += 'log(p): prompt\\n'\n",
    "        s += '----------------\\n'\n",
    "        for prompt, score in list(zip(prompts, scores))[:10]:\n",
    "            s += f'{score:.2f}: {prompt}\\n'\n",
    "        return s\n",
    "    \n",
    "special_output_token = '[[[[OUTPUT]]]]'\n",
    "def get_query(prompt, eval_template, input_, output_, demo_data, demos_template):\n",
    "    \"\"\"\n",
    "    Returns the text sent to the LLM for likelihood evaluation.\n",
    "    Parameters:\n",
    "        prompt: The prompt.\n",
    "        eval_template: The template for the evaluation queries.\n",
    "        input_: The input.\n",
    "        output_: The output.\n",
    "    Returns:\n",
    "        The query for the LLM and the range of the output text in the form of (start_idx, end_idx).\n",
    "    \"\"\"\n",
    "    demos = demos_template.fill(demo_data)\n",
    "    query = eval_template.fill(prompt=prompt,\n",
    "                               input=input_,\n",
    "                               output=output_,\n",
    "                               full_demo=demos)\n",
    "    query_without_output = eval_template.fill(prompt=prompt,\n",
    "                                              input=input_,\n",
    "                                              output=special_output_token,\n",
    "                                              full_demo=demos)\n",
    "\n",
    "    first_idx = query_without_output.find(special_output_token)\n",
    "    output_idx = first_idx, first_idx + len(output_)\n",
    "    return query, output_idx\n",
    "\n",
    "def likelihood_evaluator(prompts, eval_template, eval_data, demos_template, few_shot_data, config):\n",
    "    \"\"\"\n",
    "    For each prompt, evaluate the likelihood of the data (output) given the prompt.\n",
    "    Parameters:\n",
    "        prompts: A list of prompts.\n",
    "        eval_template: The template for the evaluation queries.\n",
    "        eval_data: The data to use for evaluation.\n",
    "        config: The configuration dictionary.\n",
    "    Returns:\n",
    "        A LikelihoodEvaluationResult object.\n",
    "    \"\"\"\n",
    "    queries = []\n",
    "    output_indices = []\n",
    "    for prompt in prompts:\n",
    "        subsampled_data = subsample_data(\n",
    "            eval_data, config['num_samples'])\n",
    "        for d in zip(*subsampled_data):\n",
    "            input_, output_ = d\n",
    "            demo_data = subsample_data(\n",
    "                few_shot_data, config['num_few_shot'])\n",
    "            query, output_idx = get_query(\n",
    "                prompt, eval_template, input_, output_, demo_data, demos_template)\n",
    "            queries.append(query)\n",
    "            output_indices.append(output_idx)\n",
    "\n",
    "    log_probs, _ = model.log_probs(queries, output_indices)\n",
    "\n",
    "    res = LikelihoodEvaluationResult(prompts, log_probs, config['num_samples'])\n",
    "\n",
    "    return res\n",
    "\n",
    "def get_eval_method(eval_method):\n",
    "    \"\"\"\n",
    "    Returns the evaluation method object.\n",
    "    Parameters:\n",
    "        eval_method: The evaluation method to use. ('likelihood')\n",
    "    Returns:\n",
    "        An evaluation method object.\n",
    "    \"\"\"\n",
    "    if callable(eval_method):\n",
    "        return eval_method\n",
    "    if eval_method == 'likelihood':\n",
    "        return likelihood_evaluator\n",
    "    elif eval_method == 'bandits':\n",
    "        return bandits_evaluator\n",
    "    else:\n",
    "        raise ValueError('Invalid evaluation method.')\n",
    "\n",
    "\n",
    "def evalute_prompts(prompts, eval_template, eval_data, demos_template, few_shot_data, eval_method, config):\n",
    "    \"\"\"\n",
    "    Returns the scores for a list of prompts.\n",
    "    Parameters:\n",
    "        prompts: A list of prompts.\n",
    "        eval_template: The template for the evaluation queries.\n",
    "        eval_data: The data to use for evaluation.\n",
    "        eval_method: The evaluation method to use. ('likelihood')\n",
    "        config: The configuration dictionary.\n",
    "    Returns:\n",
    "        An evaluation result object.\n",
    "    \"\"\"\n",
    "    eval_method = get_eval_method(eval_method)\n",
    "    return eval_method(prompts, eval_template, eval_data, demos_template, few_shot_data, config)\n",
    "\n",
    "\n",
    "def demo_function(eval_template, config):\n",
    "    \"\"\"\n",
    "    Returns a function that can be manually test the LLM with a chosen prompt.\n",
    "    Parameters:\n",
    "        eval_template: The template for the evaluation queries.\n",
    "        config: The configuration dictionary.\n",
    "    Returns:\n",
    "        A function that takes a prompt and returns a demo.\n",
    "    \"\"\"\n",
    "    def fn(prompt, inputs):\n",
    "        if not isinstance(inputs, list):\n",
    "            inputs = [inputs]\n",
    "        queries = []\n",
    "        for input_ in inputs:\n",
    "            query = eval_template.fill(prompt=prompt, input=input_)\n",
    "            queries.append(query)\n",
    "        outputs = model.generate_text(\n",
    "            queries, n=1)\n",
    "        return [out.strip().split('\\n')[0] for out in outputs]\n",
    "\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c20dfe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_split(data, split_size):\n",
    "    \"\"\"\n",
    "    Split data into two parts. Data is in the form of a tuple of lists.\n",
    "    \"\"\"\n",
    "    inputs, outputs = data\n",
    "    assert len(inputs) == len(outputs)\n",
    "    indices = random.sample(range(len(inputs)), split_size)\n",
    "    inputs1 = [inputs[i] for i in indices]\n",
    "    outputs1 = [outputs[i] for i in indices]\n",
    "    inputs2 = [inputs[i] for i in range(len(inputs)) if i not in indices]\n",
    "    outputs2 = [outputs[i] for i in range(len(inputs)) if i not in indices]\n",
    "    return (inputs1, outputs1), (inputs2, outputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de6d27b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_config(config, base_config='configs/default.yaml'):\n",
    "    # Get default config from yaml\n",
    "    with open(os.path.join(os.path.dirname(__file__), base_config)) as f:\n",
    "        default_config = yaml.safe_load(f)\n",
    "\n",
    "    # Update default config with user config\n",
    "    # Note that the config is a nested dictionary, so we need to update it recursively\n",
    "    def update(d, u):\n",
    "        for k, v in u.items():\n",
    "            if isinstance(v, dict):\n",
    "                d[k] = update(d.get(k, {}), v)\n",
    "            else:\n",
    "                d[k] = v\n",
    "        return d\n",
    "\n",
    "    return update(default_config, config)\n",
    "\n",
    "\n",
    "def simple_config(eval_model, prompt_gen_model, prompt_gen_mode, num_prompts, eval_rounds, prompt_gen_batch_size, eval_batch_size):\n",
    "    \"\"\"Returns a config and splits the data into sensible chunks.\"\"\"\n",
    "    conf = update_config({}, 'configs/bandits.yaml')\n",
    "    conf['generation']['model']['gpt_config']['model'] = prompt_gen_model\n",
    "    if prompt_gen_mode == 'insert':\n",
    "        conf['generation']['model']['name'] = 'GPT_insert'\n",
    "        conf['generation']['model']['batch_size'] = 1\n",
    "    elif prompt_gen_mode == 'forward':\n",
    "        conf['generation']['model']['name'] = 'GPT_forward'\n",
    "        conf['generation']['model']['batch_size'] = prompt_gen_batch_size\n",
    "    conf['generation']['num_subsamples'] = num_prompts // 10\n",
    "    conf['generation']['num_prompts_per_subsample'] = 10\n",
    "\n",
    "    conf['evaluation']['base_eval_config']['model']['gpt_config']['model'] = eval_model\n",
    "    conf['evaluation']['base_eval_config']['model']['batch_size'] = eval_batch_size\n",
    "    # total eval = rounds * num_prompts_per_round * num_samples\n",
    "    # We fix the number of samples to 10 and the number of prompts per round to 1/3 of\n",
    "    # the total number of prompts. We then set the number of rounds to be the number of\n",
    "    # prompts divided by the number of prompts per round.\n",
    "    conf['evaluation']['num_prompts_per_round'] = 0.334\n",
    "    conf['evaluation']['rounds'] = eval_rounds\n",
    "    conf['evaluation']['base_eval_config']['num_samples'] = 5\n",
    "    # In this simple demo, there is no dataset splitting, so we just use the same data for prompt generation and evaluation\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99ac7f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_prompt_gen_template(prompt_gen_template, prompt_gen_mode):\n",
    "    if prompt_gen_template is None:\n",
    "        if prompt_gen_mode == 'forward':\n",
    "            prompt_gen_template = \"I gave a friend an instruction. Based on the instruction they produced the following input-output pairs:\\n\\n[full_DEMO]\\n\\nThe instruction was to [APE]\"\n",
    "        elif prompt_gen_mode == 'insert':\n",
    "            prompt_gen_template = None\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                'Invalid prompt_gen_mode: {}'.format(prompt_gen_mode))\n",
    "    return prompt_gen_template\n",
    "\n",
    "\n",
    "def simple_ape(dataset,\n",
    "               eval_template='Instruction: [PROMPT]\\nInput: [INPUT]\\nOutput: [OUTPUT]',\n",
    "               prompt_gen_template=None,\n",
    "               demos_template='Input: [INPUT]\\nOutput: [OUTPUT]',\n",
    "               eval_model='text-davinci-002',\n",
    "               prompt_gen_model='text-davinci-002',\n",
    "               prompt_gen_mode='forward',\n",
    "               num_prompts=50,\n",
    "               eval_rounds=20,\n",
    "               prompt_gen_batch_size=200,\n",
    "               eval_batch_size=500):\n",
    "    \"\"\"\n",
    "    Function that wraps the find_prompts function to make it easier to use.\n",
    "    Design goals: include default values for most parameters, and automatically\n",
    "    fill out the config dict for the user in a way that fits almost all use cases.\n",
    "\n",
    "    The main shortcuts this function takes are:\n",
    "    - Uses the same dataset for prompt generation, evaluation, and few shot demos\n",
    "    - Uses UCB algorithm for evaluation\n",
    "    - Fixes the number of prompts per round to num_prompts // 3  (so the first three rounds will\n",
    "        sample every prompt once)\n",
    "    - Fixes the number of samples per prompt per round to 5\n",
    "    Parameters:\n",
    "        dataset: The dataset to use for evaluation.\n",
    "        eval_template: The template for the evaluation queries.\n",
    "        prompt_gen_template: The template to use for prompt generation.\n",
    "        demos_template: The template for the demos.\n",
    "        eval_model: The model to use for evaluation.\n",
    "        prompt_gen_model: The model to use for prompt generation.\n",
    "        prompt_gen_mode: The mode to use for prompt generation.\n",
    "        num_prompts: The number of prompts to generate during the search.\n",
    "        eval_rounds: The number of evaluation rounds to run.\n",
    "    Returns:\n",
    "        An evaluation result and a function to evaluate the prompts with new inputs.\n",
    "    \"\"\"\n",
    "    prompt_gen_template = get_simple_prompt_gen_template(\n",
    "        prompt_gen_template, prompt_gen_mode)\n",
    "    conf = simple_config(\n",
    "        eval_model, prompt_gen_model, prompt_gen_mode, num_prompts, eval_rounds, prompt_gen_batch_size, eval_batch_size)\n",
    "    return find_prompts(eval_template, demos_template, dataset, dataset, conf, prompt_gen_template=prompt_gen_template)\n",
    "\n",
    "\n",
    "def simple_eval(dataset,\n",
    "                prompts,\n",
    "                eval_template='Instruction: [PROMPT]\\nInput: [INPUT]\\nOutput: [OUTPUT]',\n",
    "                demos_template='Input: [INPUT]\\nOutput: [OUTPUT]',\n",
    "                eval_model='text-davinci-002',\n",
    "                num_samples=50):\n",
    "    \"\"\"\n",
    "    Function that wraps the evaluate_prompts function to make it easier to use.\n",
    "    Parameters:\n",
    "        dataset: The dataset to use for evaluation.\n",
    "        prompts: The list of prompts to evaluate.\n",
    "        eval_template: The template for the evaluation queries.\n",
    "        demos_template: The template for the demos.\n",
    "        eval_model: The model to use for evaluation.\n",
    "    Returns:\n",
    "        An evaluation result.\n",
    "    \"\"\"\n",
    "    eval_template = EvalTemplate(eval_template)\n",
    "    demos_template = DemosTemplate(demos_template)\n",
    "    conf = update_config({}, 'configs/default.yaml')\n",
    "    conf['evaluation']['model']['gpt_config']['model'] = eval_model\n",
    "    conf['evaluation']['num_samples'] = min(len(dataset[0]), num_samples)\n",
    "    res = evalute_prompts(\n",
    "        prompts, eval_template, dataset, demos_template, dataset, conf['evaluation']['method'], conf['evaluation'])\n",
    "    return res\n",
    "\n",
    "def generate_prompts(prompt_gen_template, demos_template, prompt_gen_data, config):\n",
    "    \"\"\"\n",
    "    Generates prompts using the prompt generator.\n",
    "    Parameters:\n",
    "        prompt_gen_template: The template for the prompt generator queries.\n",
    "        demos_template: The template for the demonstrations.\n",
    "        prompt_gen_data: The data to use for prompt generation.\n",
    "        config: The configuration dictionary.\n",
    "    Returns:\n",
    "        A list of prompts.\n",
    "    \"\"\"\n",
    "    queries = []\n",
    "    for _ in range(config['num_subsamples']):\n",
    "        subsampled_data = subsample_data(\n",
    "            prompt_gen_data, config['num_demos'])\n",
    "        queries.append(get_query(prompt_gen_template,\n",
    "                                 demos_template, subsampled_data))\n",
    "\n",
    "    prompts = model.generate_text(\n",
    "        queries, n=config['num_prompts_per_subsample'])\n",
    "    return prompts\n",
    "\n",
    "def find_prompts(eval_template,\n",
    "                 demos_template,\n",
    "                 prompt_gen_data,\n",
    "                 eval_data,\n",
    "                 conf,\n",
    "                 base_conf='configs/default.yaml',\n",
    "                 few_shot_data=None,\n",
    "                 prompt_gen_template=None):\n",
    "    \"\"\"\n",
    "    Function to generate prompts using APE.\n",
    "    Parameters:\n",
    "        eval_template: The template for the evaluation queries.\n",
    "        demos_template: The template for the demos.\n",
    "        prompt_gen_data: The data to use for prompt generation.\n",
    "        eval_data: The data to use for evaluation.\n",
    "        conf: The configuration dictionary.\n",
    "        few_shot_data: The data to use for demonstrations during eval (not implemented yet).\n",
    "        eval_method: The evaluation method to use. ('likelihood')\n",
    "        prompt_gen_template: The template to use for prompt generation.\n",
    "        verbosity: The verbosity level.\n",
    "    Returns:\n",
    "        An evaluation result. Also returns a function to evaluate the prompts with new inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    conf = update_config(conf, base_conf)\n",
    "\n",
    "    # Generate prompts\n",
    "    eval_template = EvalTemplate(eval_template)\n",
    "    demos_template = DemosTemplate(demos_template)\n",
    "    if prompt_gen_template is None:\n",
    "        prompt_gen_template = eval_template.convert_to_generation_template()\n",
    "    else:\n",
    "        prompt_gen_template = GenerationTemplate(prompt_gen_template)\n",
    "\n",
    "    if few_shot_data is None:\n",
    "        few_shot_data = prompt_gen_data\n",
    "\n",
    "    print('Generating prompts...')\n",
    "    prompts = generate_prompts(\n",
    "        prompt_gen_template, demos_template, prompt_gen_data, conf['generation'])\n",
    "\n",
    "    print('Model returned {} prompts. Deduplicating...'.format(len(prompts)))\n",
    "    prompts = list(set(prompts))\n",
    "    print('Deduplicated to {} prompts.'.format(len(prompts)))\n",
    "\n",
    "    print('Evaluating prompts...')\n",
    "\n",
    "    res = evalute_prompts(prompts, eval_template, eval_data, demos_template, few_shot_data,\n",
    "                                   conf['evaluation']['method'], conf['evaluation'])\n",
    "\n",
    "    print('Finished evaluating.')\n",
    "\n",
    "    demo_fn = demo_function(eval_template, conf['demo'])\n",
    "\n",
    "    return res, demo_fn\n",
    "\n",
    "\n",
    "def evaluate_prompts(prompts, eval_template, eval_data, demos_template, few_shot_data, conf,\n",
    "                     base_conf='configs/default.yaml'):\n",
    "    \"\"\"\n",
    "    Function to evaluate a list of prompts.\n",
    "    Parameters:\n",
    "        prompts: The list of prompts to evaluate.\n",
    "        eval_template: The template for the evaluation queries.\n",
    "        eval_data: The data to use for evaluation.\n",
    "        eval_method: The evaluation method to use. ('likelihood')\n",
    "        conf: The configuration dictionary.\n",
    "        base_conf: The base configuration file.\n",
    "    Returns:\n",
    "        A list of prompts and their scores, sorted by score.\n",
    "    \"\"\"\n",
    "\n",
    "    conf = update_config(conf, base_conf)\n",
    "\n",
    "    # Generate prompts\n",
    "    eval_template = EvalTemplate(eval_template)\n",
    "    demos_template = DemosTemplate(demos_template)\n",
    "\n",
    "    print('Evaluating prompts...')\n",
    "    res = evalute_prompts(\n",
    "        prompts, eval_template, eval_data, demos_template, few_shot_data, conf['evaluation']['method'],\n",
    "        conf['evaluation'])\n",
    "\n",
    "    print('Finished evaluating.')\n",
    "\n",
    "    return res\n",
    "\n",
    "def get_generation_query(eval_template,\n",
    "                         demos_template,\n",
    "                         conf,\n",
    "                         prompt_gen_data,\n",
    "                         prompt_gen_template=None,\n",
    "                         num_query=1):\n",
    "    # Generate prompts\n",
    "    eval_template = EvalTemplate(eval_template)\n",
    "    demos_template = DemosTemplate(demos_template)\n",
    "    if prompt_gen_template is None:\n",
    "        prompt_gen_template = eval_template.convert_to_generation_template()\n",
    "    else:\n",
    "        prompt_gen_template = GenerationTemplate(prompt_gen_template)\n",
    "\n",
    "    # First, generate a few prompt queries:\n",
    "    queries = []\n",
    "    for _ in range(num_query):\n",
    "        subsampled_data = subsample_data(\n",
    "            prompt_gen_data, conf['generation']['num_demos'])\n",
    "        queries.append(get_query(prompt_gen_template,\n",
    "                                          demos_template, subsampled_data))\n",
    "\n",
    "    return queries\n",
    "\n",
    "\n",
    "def get_evaluation_query(eval_template,\n",
    "                         demos_template,\n",
    "                         conf,\n",
    "                         eval_data,\n",
    "                         few_shot_data,\n",
    "                         eval_query=None,\n",
    "                         num_query=1\n",
    "                         ):\n",
    "    eval_template = EvalTemplate(eval_template)\n",
    "    demos_template = DemosTemplate(demos_template)\n",
    "\n",
    "    if conf['evaluation']['method'] == 'bandits':\n",
    "        eval_base_method = conf['evaluation']['base_eval_method']\n",
    "        num_few_shot = conf['evaluation']['base_eval_config']['num_few_shot']\n",
    "    else:\n",
    "        eval_base_method = conf['evaluation']['method']\n",
    "        num_few_shot = conf['evaluation']['num_few_shot']\n",
    "\n",
    "    if eval_query is None:\n",
    "        if eval_base_method == 'likelihood':\n",
    "            eval_query = get_query\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                'Cannot estimate costs for: {}'.format(eval_base_method))\n",
    "\n",
    "    max_prompt_len = conf['generation']['model']['gpt_config']['max_tokens']\n",
    "    filler_prompt = 'GGGG' * max_prompt_len\n",
    "\n",
    "    queries = []\n",
    "    for _ in range(num_query):\n",
    "        idx = random.randint(0, len(eval_data[0]) - 1)\n",
    "        input_, output_ = eval_data[0][idx], eval_data[1][idx]\n",
    "        demo_data = subsample_data(few_shot_data, num_few_shot)\n",
    "        query = eval_query(filler_prompt, eval_template, input_,\n",
    "                           output_, demo_data, demos_template)[0]\n",
    "        queries.append(query)\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96e6115e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.07s/it]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized prompt: Is the sentiment of the following text positive or negative?\n",
      "Embedding: (-0.011109919287264347, 0.3535281717777252, -0.0517016164958477, 0.02186959609389305, -0.10469207912683487, -0.17762413620948792, 0.030455095693469048, -0.06047740951180458, 0.24406816065311432, -0.014500077813863754, 0.18065515160560608, 0.11902257055044174, 0.15227621793746948, 0.121745266020298, 0.03753717243671417, 0.024582503363490105, 0.05646296590566635, -0.22020116448402405, 0.047522012144327164, -0.024562573060393333, -0.06697848439216614, 0.12067969143390656, 0.032858602702617645, 0.04359618201851845, -0.060819435864686966, 0.2984614372253418, -0.133926659822464, 0.15761683881282806, 0.017571687698364258, -0.6304754614830017, -0.2724848687648773, 0.05615871399641037, 0.0970522090792656, 0.044642701745033264, -0.10183541476726532, -0.04565545916557312, 0.028056440874934196, -0.027529120445251465, 0.04863203316926956, 0.17135927081108093, -0.19201165437698364, -0.10806091129779816, 0.16634328663349152, 0.023756835609674454, 0.17342419922351837, -0.0277717188000679, -0.17276997864246368, 0.059562668204307556, 0.06340494751930237, -0.2393387109041214, -0.06515713036060333, -0.06802517175674438, -0.04244399815797806, -0.1726187765598297, -0.16271767020225525, 0.27482321858406067, -0.0416647233068943, -0.04429756850004196, 0.053202446550130844, -0.023624198511242867, 0.20125477015972137, -0.21515913307666779, -0.14660336077213287, -0.12815739214420319, -0.07277923077344894, -0.24022233486175537, -0.25364744663238525, -0.09597878158092499, -0.4813598394393921, 0.1496424376964569, 0.24147818982601166, 0.05125752091407776, -0.027980558574199677, 0.11184193193912506, 0.029656128957867622, 0.0190524123609066, -0.0635363981127739, -0.21699665486812592, -0.031940359622240067, -0.03529626503586769, -0.16815264523029327, 0.0486101396381855, 0.027291974052786827, 0.3121829032897949, 0.06722797453403473, -0.2725563645362854, -0.013637697324156761, -0.040703244507312775, -0.1991090625524521, 0.19895127415657043, -0.12833815813064575, -0.21851412951946259, 0.07975253462791443, 0.039203330874443054, -0.6134369373321533, 0.1266302466392517, -0.3465495705604553, -0.09399117529392242, -0.13389483094215393, 5.890482425689697, 0.14812009036540985, 0.15528090298175812, -0.009759146720170975, 0.06507101655006409, -0.17758776247501373, 0.08691171556711197, 0.0010760670993477106, -0.02092033065855503, 0.10182436555624008, -0.11313183605670929, -0.0196326095610857, 0.005424482747912407, 0.23053203523159027, 0.07371805608272552, 0.023477375507354736, 0.2359665185213089, 0.0147691760212183, 0.24736371636390686, 0.27919241786003113, 0.2038949728012085, -0.03400196507573128, 0.03210841864347458, -0.1976856291294098, 0.09699960052967072, 0.168664813041687, -0.9266229867935181, 0.15310867130756378, -4.984221237634862e-32, 0.16979755461215973, 0.13214178383350372, 0.254787802696228, 0.05825771018862724, -0.20974688231945038, 0.08399901539087296, -0.2554774582386017, -0.08068615943193436, -0.11319047212600708, -0.2659006714820862, 0.07257642596960068, -0.07765685766935349, -0.16359812021255493, 0.04334795102477074, 0.29124194383621216, -0.2980350852012634, 0.10681850463151932, 0.017342276871204376, -0.042396292090415955, -0.10267727822065353, 0.007130177225917578, 0.2735743820667267, 0.25721171498298645, -0.10019509494304657, -0.013710249215364456, 0.11037600785493851, 0.03641359880566597, -0.07055915892124176, -0.24662920832633972, -0.3262486755847931, -0.25170356035232544, 0.34602412581443787, 0.11844530701637268, -0.14334432780742645, 0.11883564293384552, -0.15108031034469604, -0.18451964855194092, 0.008677258156239986, 0.020272888243198395, -0.1388355940580368, -0.11819954216480255, 0.30707094073295593, 0.11389061063528061, -0.33804750442504883, -0.020641859620809555, 0.15844039618968964, -0.04625481739640236, -0.042335815727710724, 0.10063683986663818, 0.08771795779466629, 0.02112891897559166, 0.20894695818424225, 0.014783991500735283, 0.10859569162130356, -0.03433133289217949, -0.15318508446216583, -0.09182434529066086, 0.09790594130754471, 0.005908947438001633, 0.1357986032962799, 0.06552939116954803, -0.02192085236310959, 0.005528204143047333, -0.10217950493097305, 0.2654341161251068, 0.1214410737156868, 0.04558243975043297, -0.0001843728095991537, 0.007852206006646156, -0.04210641235113144, 0.0073935771360993385, 0.11529003083705902, -0.20076890289783478, 0.0807357057929039, -0.055696193128824234, 0.08856599777936935, -0.32209691405296326, 0.25006887316703796, 0.02281983382999897, -0.13130581378936768, 0.056428927928209305, -0.29390227794647217, 0.13693153858184814, -0.3534146845340729, 0.23652376234531403, 0.030463436618447304, 0.02752617560327053, -0.14779847860336304, -0.006013395264744759, 0.23643741011619568, -0.3522410988807678, 0.27062296867370605, 0.08400320261716843, 0.09585785120725632, -0.00942307896912098, 4.438150332811359e-32, -0.02843099646270275, -0.2775276303291321, -0.24472597241401672, -0.19431889057159424, -0.043060749769210815, 0.16244818270206451, -0.006524355616420507, 0.11268968135118484, -0.17077821493148804, 0.060747306793928146, -0.17306211590766907, 0.1515796184539795, 0.12945038080215454, 0.024865996092557907, -0.003005865728482604, -0.005629848688840866, 0.15964919328689575, -0.04058612883090973, 0.16660840809345245, -0.2987482249736786, -0.3065556585788727, -0.008959323167800903, -0.05821597948670387, 0.17264597117900848, 0.17941509187221527, 0.29590752720832825, 0.08927935361862183, 0.11592064052820206, 0.026470830664038658, -0.06663476675748825, -0.02675936557352543, -0.03780880570411682, 0.08900116384029388, 0.25455716252326965, -0.09831111133098602, -0.005421874113380909, -0.11863643676042557, -0.28219616413116455, 0.22605666518211365, 0.19351710379123688, 0.24640512466430664, 0.06588868051767349, 0.026288457214832306, -0.042413145303726196, -0.1996307671070099, -0.0024366136640310287, -0.04496779292821884, -0.05699959024786949, -0.08538524061441422, -0.11699800193309784, -0.31326714158058167, -0.09070678055286407, 0.14200665056705475, 0.12108216434717178, -0.04148964583873749, -0.2662859559059143, -0.21542136371135712, -0.15666282176971436, -0.029073381796479225, -0.15134568512439728, -0.2183310091495514, 0.23316559195518494, -0.050000011920928955, -0.10288025438785553, -0.3652452528476715, -0.18357738852500916, 0.05517911538481712, -0.01932046189904213, -0.00968145951628685, -0.04974132031202316, 0.11819148808717728, -0.013031811453402042, -0.1854717880487442, -0.03124804049730301, 0.09572877734899521, -0.022531436756253242, -0.05590564012527466, 0.002417388604953885, -0.3930617868900299, 0.024470502510666847, 0.1508074700832367, -0.023445075377821922, -0.06307388097047806, 0.12733866274356842, 0.1388411521911621, -0.13452383875846863, -0.14635616540908813, 0.0652591735124588, 0.08053991943597794, 0.15673793852329254, 0.07877860963344574, 0.2288074493408203, 0.012763578444719315, -0.02409658022224903, 0.14322832226753235, -8.864262213137408e-08, -0.24768055975437164, -0.17754557728767395, -0.28711584210395813, 0.22767512500286102, -0.021606365218758583, -0.0398670919239521, -0.10631673038005829, -0.11140753328800201, -0.05178051069378853, 0.2656641900539398, 0.2294624149799347, -0.10026311874389648, -0.07050769031047821, -0.14383037388324738, -0.0593358650803566, -0.024980124086141586, -0.0423501618206501, -0.08125639706850052, 0.322587788105011, -0.017123201861977577, 0.4323146641254425, 0.38765329122543335, -0.07736390084028244, 0.09863870590925217, -0.055447883903980255, -0.038746919482946396, -0.025495536625385284, -0.6759485602378845, -0.154200479388237, -0.0357610248029232, 0.15078936517238617, -0.11420223116874695, -0.2837863862514496, -0.01870969869196415, 0.036529574543237686, -0.10420215129852295, -0.08868513256311417, -0.09921599924564362, 0.20445357263088226, 0.030955446884036064, 0.08249293267726898, 0.2246694564819336, -0.282521516084671, -0.24844607710838318, -0.024224113672971725, -0.068654865026474, -0.14711794257164001, -0.13574053347110748, -0.14388564229011536, 0.07822363078594208, 0.05926639959216118, -0.02371865324676037, 0.016005976125597954, 0.2972605228424072, 0.029916279017925262, -0.07267218828201294, 0.07884794473648071, 0.3024634122848511, -0.17602598667144775, -0.06273078173398972, 0.07031187415122986, -0.04451432451605797, -0.015490121208131313, -0.23108021914958954)\n",
      "Evaluation score: -3.3668532674256646\n"
     ]
    }
   ],
   "source": [
    "# Define demo data (not used in this simple example)\n",
    "demo_data = [\n",
    "        \"Example input: 'This movie was fantastic! I loved every minute of it.' Expected output: 'positive'\",\n",
    "        \"Example input: 'The service was terrible and the food was cold.' Expected output: 'negative'\",\n",
    "        \"Example input: 'I can't believe how amazing this product is!' Expected output: 'positive'\",\n",
    "        \"Example input: 'I requested a refund because it didn't work as advertised.' Expected output: 'negative'\"\n",
    "    ]\n",
    "\n",
    "task_description = \"sentiment analysis of product reviews, classifying them as positive or negative\"\n",
    "prompt_gen = partial(prompt_generation, task_description=task_description)\n",
    "\n",
    "# Initialize ZOPO with our functions\n",
    "zopo = ZOPOAlgorithm(\n",
    "    prompt_generation=prompt_generation,\n",
    "    nlp_embedding=nlp_embedding,\n",
    "    inverse_embedding=inverse_embedding,\n",
    "    projection_function=projection_function,\n",
    "    evaluation_function=evaluation_function,\n",
    "    m=5,      # Number of initial prompt candidates\n",
    "    T=3,      # Number of optimization iterations\n",
    "    delta=0.1, # Confidence parameter\n",
    "    kappa=1.0, # Kernel bound\n",
    "    alpha=0.1  # Step size\n",
    ")\n",
    "\n",
    "# Run the algorithm\n",
    "best_prompt = zopo.run(demo_data)\n",
    "\n",
    "print(\"Optimized prompt:\", best_prompt)\n",
    "\n",
    "# Show the embedding for the best prompt\n",
    "best_embedding = nlp_embedding(best_prompt, zopo.embedding_model, zopo.embedding_tokenizer)\n",
    "print(\"Embedding:\", best_embedding)\n",
    "\n",
    "# Show the evaluation score\n",
    "best_score = evaluation_function(best_embedding)\n",
    "print(\"Evaluation score:\", best_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
